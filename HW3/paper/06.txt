       ARTICLE

       https://doi.org/10.1038/s42003-022-03036-1 OPEN
       Brains and algorithms partially converge in natural
       language processing
                              ✉                       ✉
       Charlotte Caucheteux1,2  & Jean-Rémi King    1,3


       Deep learning algorithms trained to predict masked words from large amount of text have
       recently been shown to generate activations similar to those of the human brain. However,
       what drives this similarity remains currently unknown. Here, we systematically compare a
       variety of deep language models to identify the computational principles that lead them to
       generate brain-like representations of sentences. Speciﬁcally, we analyze the brain responses
       to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with


1234567890():,; functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We
       then test where and when each of these algorithms maps onto the brain responses. Finally,
       we estimate how the architecture, training, and performance of these models independently
       account for the generation of brain-like representations. Our analyses reveal two main
       ﬁndings. First, the similarity between the algorithms and the brain primarily depends on their
       ability to predict words from context. Second, this similarity reveals the rise and maintenance
       of perceptual, lexical, and compositional representations within each cortical region. Overall,
       this study shows that modern language algorithms partially converge towards brain-like
       solutions, and thus delineates a promising path to unravel the foundations of natural language
       processing.


       1 Facebook AI Research, Paris, France. 2 Université Paris-Saclay, Inria, CEA, Palaiseau, France. 3 École normale supérieure, PSL University, CNRS, Paris, France.
       ✉email: ccaucheteux@fb.com; jeanremi@fb.com

       COMMUNICATIONS BIOLOGY |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio 1
ARTICLE                                          COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1

      eep learning algorithms have recently made considerable magneto-encephalography (MEG). During these two 1 h-long
      progress in developing abilities generally considered sessions the subjects read isolated Dutch sentences composed of
D                            1–3                               37
      unique to the human species . Language transformers, 9–15 words . After quantifying the signal-to-noise ratio of the
in particular, can complete, translate, and summarize texts with brain responses (Fig. 2), we train a variety of deep learning
an unprecedented accuracy4–7. These advances raise a major algorithms, extract their responses to the very same sentences and
question: do these algorithms process words and sentences like compare their ability to linearly map onto the fMRI and MEG
the human brain?                                      brain recordings. Finally, we assess how the training, the archi-
  Recent neuroimaging studies suggest that they might—at least tecture, and the word-prediction performance independently
partially8–12. First, word embeddings—high dimensional dense explains the brain-similarity of these algorithms and localize this
vectors trained to predict lexical neighborhood13–16—have been convergence in both space and time.
shown to linearly map onto the brain responses elicited by words We ﬁnd that (1) a variety of deep learning algorithms linearly
presented either in isolation17–19 or within narratives20–30. Sec- map onto the brain areas associated with reading (Fig. 3), (2) the
ond, the contextualized activations of language transformers best brain-mapping are obtained from the middle layers of deep
improve the precision of this mapping, especially in the pre- language models and, critically, we show that (3) whether an
frontal, temporal and parietal cortices31–33. Third, speciﬁc com- algorithm maps onto the brain primarily depends on its ability to
putations of deep language models, such as the estimations of predict words context (Fig. 4).
word surprisal (i.e., the probability of a word given its context)
and the parsing of syntactic constituents have been shown to
correlate with evoked related potentials30,34–36 and functional Results
magnetic resonance imaging (fMRI)28,36. However, the above Shared brain responses to words and sentences across subjects.
studies remain fragmentary: ﬁrst, most only analyze a small Before comparing deep language models to brain activity, we ﬁrst
number of subjects (although see refs. 20,28,29). Second, most aim to identify the brain regions recruited during the reading of
studies only explore the spatial but not the temporal properties of sentences. To this end, we (i) analyze the average fMRI and MEG
the brain responses to language (although see refs. 30,33). responses to sentences across subjects and (ii) quantify the signal-
  More critically, the principles that lead a deep language models to-noise ratio of these responses, at the single-trial single-voxel/
to generate brain-like representations remain largely unknown. sensor level.
Indeed, past studies only investigated a small set of pretrained As expected38–41, the average fMRI and MEG responses to
language models that typically vary in dimensionality, archi- words reveals a hierarchy of neural responses originating in V1
tecture, training objective, and training corpus. The inherent around 100 ms and continuing within the left posterior fusiform
correlations between these multiple factors thus prevent identi- gyrus around 200 ms, the superior and middle temporal gyri, as
fying those that lead algorithms to generate brain-like well as the pre-motor and infero-frontal cortices between 150 and
representations.                                      500 ms after word onset (Supplementary Movie 1 and Supple-
  To address this issue, we systematically compare a wide variety mentary Note 1 and Fig. 2a).
of deep language models in light of human brain responses to To quantify the proportion of these brain responses that
sentences (Fig. 1). Speciﬁcally, we analyze the brain activity of 102 depend on the speciﬁc content of sentences, we ﬁt, for each
healthy adults, recorded with both fMRI and source-localized subject separately, a shared response model across subjects


Fig. 1 Approach. a Subjects read isolated sentences while their brain activity was recorded with fMRI and MEG37. b To compute the similarity between a
deep language model and the brain, we (1) ﬁt a linear regression W from the model’s activations X to predict brain responses Y and (2) evaluate this

mapping with a correlation between the predicted and true brain responses to held-out sentences Ytest. c We consider different types of embedding
depending on whether they vary with neighboring words during training and/or during inference. Visual embeddings refer, here, to the activations of a deep
convolutional neural network trained on character recognition. Lexical embeddings refer, here, to the non-contextualized activations associated with a word
independently of its context. Here, we use the word-embedding layer of language transformers (bottom green), as opposed to algorithms like Word2Vec93
(middle, green). Compositional embeddings refer, here, to the context-dependent activations of a deep language model (see SI.4 for a discussion of our
terminology). d The three panels represent three hypotheses on the link between deep language models and the brain. Each dot represents one embedding.
Algorithm are said to converge to brain-like computations if their performance (x-axis: i.e., accuracy at predicting a word from its previous context) indexes
their ability to map onto brain responses to the same stimuli (i.e., y-axis: brain score). High-dimensional neural networks can, in principle, capture relevant
information94,95 and thus lead to a fortunate similarity with brain responses, and event a systematic divergence.

2                  COMMUNICATIONS BIOLOGY |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1                           ARTICLE

   a              Average response                    associated with language processing elicits representations
                                                      speciﬁc to words and sentences17,43,44.

                                                      Deep language models reveal the hierarchical generation of
                                                      language representations in the brain. Where and when are the
                                                      language representations of the brain similar to those of deep
                                                      language models? To address this issue, we extract the activations
                                                      (X) of a visual, a word and a compositional embedding (Fig. 1d)
                                                      and evaluate the extent to which each of them maps onto the
   b                   Noise ceiling                  brain responses (Y) to the same stimuli. To this end, we ﬁt, for
                                                                                ℓ
                                                      each subject independently, an 2-penalized regression (W)to
                                                      predict single-sample fMRI and MEG responses for each voxel/
                                                      sensor independently. We then assess the accuracy of this map-
                                                      ping with a brain-score similar to the one used to evaluate the
                                                      shared response model.
                                                        Overall, the brain scores of these trained models are largely
                                                      above chance (all p <10−9, Fig. 4a, e). The modest correlation
                                                      values are consistent with the high level of noise in single-sample
                                                      single-voxel/channel neuroimaging data (Fig. 2b–d). For example,
                                                      fMRI and MEG scores reach R = 0.048 and R = 0.041, respec-
   c                                                  tively, for the compositional embedding, which is close to and
                                                      even exceeds our shared response model (fMRI: R = 0.060, MEG:
                                                      R = 0.020, Fig. 2).
                                                        In fMRI, the brain scores of the visual embedding peak in the
                                                      early visual cortex (V1) (mean brain scores across voxels:
                                                      R = 0.022 ± 0.003, p <10−11). By contrast, the brain scores of
                                                      lexical embedding peak in the left superior temporal gyrus
                                                      (R = 0.052 ± 0.004, p <10−13) as well as in the inferior temporal
                                                      cortex and middle frontal gyrus (R = 0.053 ± 0.003, p <10−15)
                                                      and are signiﬁcant across the entire language and reading
                                                      network (Fig. 3b). Finally, the brain scores of the compositional
                                                      embedding are signiﬁcantly higher than those of lexical of
   d                                                  embeddings in the superior temporal gyrus (ΔR = 0.012 ± 0.001,
                                                      p <10−16), the angular gyrus (ΔR = 0.010 ± 0.001, p <10−16), the
                                                      infero-frontal cortex (ΔR = 0.016 ± 0.001, p <10−16) and the
                                                      dorsolateral prefrontal cortex (ΔR = 0.012 ± 0.001, p <10−13).
                                                      While these effects are lateralized (left hemisphere versus right
                                                      hemisphere: ΔR = 0.010 ± 0.001, p <10−14), they are signiﬁcant
                                                      across a remarkably large number of bilateral areas (Fig. 3b).
                                                      Lexical and compositional embeddings accurately predict brain
                                                      responses in the early visual cortex. This result is not necessarily
Fig. 2 Average and shared response modeling (or noise ceiling). a Grand surprising: language embeddings encode features (e.g., position of
average MEG source estimates to word onset (t = 0 ms) for seven regions words in the sentence, beginning/end of the sentence) that
typically associated with reading (V1: purple, M1: green, fusiform gyrus: correlate with visual information (words are ﬂashed at a screen,
dark blue, supramarginal gyrus: light blue, superior temporal gyrus: orange, and the sentences are separated by pauses). Critically, the gain
infero-frontal gyrus: yellow and fronto-polar gyrus: red), normalized to their (ΔR) of these embeddings remain very small, suggesting that this
peak response. Vertical bars indicate the peak time of each region. The full effect is mainly driven by the covariance between low-level and
(not normalized) spatio-temporal time course of the whole-brain activity is high-level representations of words.
displayed in Supplementary Movie 1. b MEG shared response model (or
noise ceilings), approximated by predicting brain responses of a given
                                                      Tracking the sequential generation of language representations
subject from those of all other subjects. Colored lines depict the mean noise
                                                      over time and space. To characterize the dynamics of these brain
ceiling in each region of interest. The gray line depicts the best noise ceiling
                                                      representations, we perform the same analysis using source-
across sources. c Same as b in sensor space. d Shared response model of
                                                      localized MEG recordings. The resulting brain scores are con-
fMRI recordings.
                                                      sistent with—although less spatially precise than—the above
                                                      fMRI results (Fig. 3c, average brain score between 0 and 2 s). For
(or noise-ceiling, see “Methods” section, Supplementary Note 2 clarity, Fig. 3d and Supplementary Movie 2 plot the gain in MEG
and Supplementary Table 1 and Fig. 2b–d). We then assess the scores: i.e., the difference of prediction performance between i)
accuracy of this model with a Pearson R correlation (hereafter word and visual embeddings (green) and ii) the difference
referred to as “brain score” following42) between the true and the between compositional and word embedding (red). The brain
predicted brain responses to held-out sentences, using a ﬁve-fold scores of the visual embedding peak around 100 ms in V1
cross-validation. Finally, we assess the statistical signiﬁcance of (R = 0.008 ± 0.002, p <10−3), and rapidly propagate to higher-
these brain scores with a two-sided Wilcoxon test across subjects, level areas (Fig. 3d and Supplementary Movie 2). The gain
after testing for multiple comparison using false discovery rate achieved by the word embedding can be observed in the left
(FDR) across voxels (see “Methods” section). Our shared posterior fusiform gyrus around 200 ms and peaks around 400 ms
response model conﬁrms that the brain network classically and in the left temporal and frontal cortices. Finally, the gain

COMMUNICATIONS BIOLOGY |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio 3
ARTICLE                                          COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1

                ab                                                    c


     Visual 
      CNN


     Lexical                                                          d
      word 
   embedding


   Compositional 
    middle layer


Fig. 3 Brain-score comparison across embeddings. Lexical and compositional representations (see Supplementary Note 4 for the deﬁnition of
compositionality) can be isolated from (i) the word embedding layer (green) and (ii) one middle layer (red) of a typical language transformer (here, the
ninth layer of a 12-layer causal transformer), respectively. We also report the brain scores of a convolutional neural network trained on visual character
recognition (blue) to account for low-level visual representations. a Mean (across subjects) fMRI scores obtained with the visual, word, and compositional
embeddings. All colored regions display signiﬁcant fMRI scores across subjects (n = 100) after false discovery rate (FDR) correction. b Mean MEG scores
averaged across all time samples and subjects (n = 95 subjects). c Left: mean MEG scores averaged across all sensors. Right: mean MEG gains averaged
across all sensors: i.e., the gain in MEG score of one level relative to the level below (blue: R[visual]; green: R[word] − R[visual]; red: R[compositional] −
R[word]). d Mean MEG gains in four regions of interest. For a whole-brain depiction of the MEG gains, see Supplementary Movie 2. For the raw scores
(without subtraction), see Supplementary Fig. 6. For the distribution of scores across channels and voxels, see Supplementary Fig. 4.

achieved by the compositional embedding is observed in a large (fMRI: ΔR=.031 ± .001, p <10−18, MEG: ΔR=.009 ± .001,
number of bilateral brain regions, and peaks around 1 s after p <10−17). For simplicity, we refer to “middle layers” as the
                                                            ∈                                        ﬁ
word onset (Fig. 3c, d).                              layers l [nlayers/2, 3nlayers/4] in Fig. 4a, e. This result con rms
  After that period, brain areas outside the language network, that the intermediary representations of deep language transfor-
such as area V1, appear to be better predicted by word and mers are more brain-like than those of the input and output
compositional embeddings than by visual ones (e.g., between layers33.
visual and word in V1: ΔR = 0.016 ± 0.002, p <10−10). These
effects could thus reﬂect feedback activity45 and explain why the
corresponding fMRI responses are better accounted for by word The emergence of brain-like representations predominantly
and compositional embeddings than by visual ones.     depends on the algorithm’s ability to predict missing words.
  Together with Supplementary Fig. 1, these results show with The above ﬁndings result from trained neural networks. However,
unprecedented spatio-temporal precision, that the brain-mapping recent studies suggest that random (i.e., untrained) networks can
of our three representative embeddings automatically recovers the signiﬁcantly map onto brain responses27,46,47. To test whether
hierarchy of visual, lexical, and compositional representations of brain mapping speciﬁcally and systematically depends on the
language in each cortical region.                     language proﬁciency of the model, we assess the brain scores of
                                                      each of the 32 architectures trained with 100 distinct amounts of
                                                      data. For each of these training steps, we compute the top-1
Compositional embeddings best predict brain responses. What accuracy of the model at predicting masked or incoming words
computational principle leads these deep language models to from their contexts. This analysis results in 32,400 embeddings,
generate brain-like activations? To address this issue, we gen- whose brain scores can be evaluated as a function of language
eralize the above analyses and evaluate the brain scores of 36 performance, i.e., the ability to predict words from context
transformer architectures (varying from 4 to 12 layers, each (Fig. 4b, f).
ranging from 128 to 512 dimensions, and each beneﬁting from 4 We observe three main ﬁndings. First, random embeddings
to 8 attention heads), trained on the same Wikipedia dataset systematically lead to signiﬁcant brain scores across subjects and
either with a causal language modeling (CLM) or a masked architectures. The mean fMRI score across voxels is
language modeling task (MLM). While causal language models R = 0.019 ± 0.001, p <10−16. The mean MEG score across
are trained to predict a word from its previous context, masked channels and time sample is R = 0.018 ± 0.0008, p <10−16. This
language models are trained to predict a randomly masked word result suggests that language transformers partially map onto
from its both left and right context.                 brain responses independently of their language abilities.
  Overall, we observe that the corresponding brain scores largely Second, brain scores strongly correlate with language accuracy
vary as a function of the relative depth of the embedding within in both MEG (R = 0.77 Pearson’s correlation on average ± 0.01
the language transformer. Speciﬁcally, both MEG and fMRI across subjects) and fMRI (R = 0.57 ± 0.02, Fig. 4b, c). The
scores follow an inverted U-shaped pattern across layers for all correlation is highest for middle (fMRI: R = 0.81 ± 0.02; MEG:
architectures (Fig. 4a, e): the middle layers systematically R = 0.86 ± 0.01) than input (fMRI: R = 0.39 ± 0.03; MEG:
outperform the output (fMRI: ΔR = 0.011 ± 0.001, p <10−18, R = 0.73 ± 0.02) and output layers (fMRI: R = 0.63 ± 0.03;
MEG:  ΔR = 0.003 ± 0.0005, p <10−13) and the input layers MEG:R = 0.78 ± 0.02). Beta coefﬁcients for each particular layer

4                  COMMUNICATIONS BIOLOGY |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1                           ARTICLE


Fig. 4 Language transformers tend to converge towards brain-like representations. a Bar plots display the average MEG score (across time and
channels) of six representative transformers varying in tasks (causal vs. masked language modeling) and depth (4–12 layers). The green and red bars
correspond to the word-embedding and middle layers, respectively. The star indicates the layer with the highest MEG score. b Average MEG scores
(across subjects, time, and channels) of each of the embeddings (dots) extracted from 18 causal architectures, separately for the input layer (word
embedding, green) and the middle layers (red). c Zoom of b, focusing on the best neural networks (i.e., word-prediction accuracy >35%). The results reveal
a plateau and/or a divergence of the middle and input layers. d Permutation importance quantiﬁes the extent to which each property of the language
transformers speciﬁcally contribute to making its embeddings more-or-less similar to brain activity (ΔR). All properties (training task. dimensionality etc.)
signiﬁcantly contribute to the brain scores (ΔR > 0, all p < 0.0001 across subjects). Ordered pairwise comparisons of the permutation scores are marked
with a star (*p < 0.05, **p < 0.01, ***p < 0.001). e–h Same as a–d, but evaluated on fMRI recordings. All error bars are the 95% conﬁdence intervals across
subjects (n = 95 for MEG, n = 100 for fMRI).

and architecture are displayed in Supplementary Fig. 1a, b. well as with several architectural variables. To disentangle the
Furthermore, single-voxel analyses show that this correlation contribution of each of these variables to the brain scores,
between brain score and language performance is driven mainly we perform a permutation feature importance analysis.
by the superior temporal sulcus and gyrus for the embedding Speciﬁcally, we train a Random Forest estimator48 to predict the
layer (mean R = 0.52 ± 0.06) and is widespread for the middle average brain scores (across voxels or MEG sensors) of each
layers, exceeding a correlation of R = 0.85 in the superior subject independently, given the layer of the representation, the
temporal sulcus, infero-frontal, fusiform and angular gyri architectural properties (number of layers, dimensionality, and
(Supplementary Fig. 1c). Overall, this result suggests that the attention head), task (CLM and MLM), amount of training
better language models are at predicting words from context, the (number of steps) and language performance (top-1 accuracy)
more their activations linearly map onto those of the brain. of the transformer. Permutation feature importance then esti-
  Third, the highest brain scores are not achieved by the very mates the unique contribution of each feature in explaining the
best language transformers (Fig. 4c, g). For instance, CLM variability of brain scores across models48,49. The results conﬁrm
transformers best map onto MEG  (R = 0.039) and fMRI  that language performance is the most important factor that
(R = 0.056) when they reach a language performance of 43% drives brain scores (Fig. 4d–h). This factor supersedes other
and 32%, respectively. By contrast, the very best transformers covarying factors such as the amount of training, and the relative
reach a language accuracy of 46%, but have signiﬁcantly smaller position of the embedding with regard to the architecture (“layer
brain scores (Fig. 4c, g).                            position”): ΔR = 0.56 ± 0.01 for fMRI, ΔR = 0.51 ± 0.02 for MEG.
                                                      Nevertheless, these other factors contribute signiﬁcantly to
                                                                                    −
Architectural and training factors impact brain scores too. the prediction of brain scores (p <10 16 across subjects for all
Language performance co-varies with the amount of training as variables).

COMMUNICATIONS BIOLOGY |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio 5
ARTICLE                                          COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1

  Overall, these results show that the ability of deep language with the ﬁndings of Schrimpf et al.27 reported simultaneously to
models to map onto the brain primarily depends on their ability ours. Together, these results suggest that deep learning algorithms
to predict words from the context, and is best supported by the converge—at least partially—to brain-like representations during
representations of their middle layers.               their training. This result is not trivial: the representations that
                                                      are optimal to predict masked or future words from large
                                                      amounts of text could have been very distinct from those the
Discussion                                            brain learns to generate.
Do deep language models and the human brain process sentences The mapping between deep language models and brain
in the same way? Following a recent methodology33,42,44,46,46,50–56, recordings reaches very low correlation values. This phenomenon
we address this issue by evaluating whether the activations of a large is expected: i) neuroimaging is notoriously noisy and ii) we
variety of deep language models linearly map onto those of 102 analyze and model here single-sample responses of single-voxel/
human brains. Our study provides two main contributions. sensor. However, the resulting brain scores are i) highly sig-
  First, our work complements previous studies26,27,30–34 and niﬁcant (all p <10−9 on average across both all fMRI voxels and
conﬁrms that the activations of deep language models sig- MEG sensors), including when compared to a permutation
niﬁcantly map onto the brain responses to written sentences baseline (Supplementary Fig. 3), and ii) in the same order of
(Fig. 3). This mapping peaks in a distributed and bilateral brain magnitude than a baseline shared-response model (or noise
network (Fig. 3a, b) and is best estimated by the middle layers of ceiling, Fig. 2) as well as previous reports (see e.g., 44 before
language transformers (Fig. 4a, e). The notion of representation correcting for the noise ceiling). Besides, we generally report brain
underlying this mapping is formally deﬁned as linearly-readable scores averaged across all voxels or MEG channels, even though
information. This operational deﬁnition helps identify brain many brain areas do not strongly respond to language (Fig. 2).
responses that any neuron can differentiate—as opposed to Critically, the link between brain scores and language perfor-
entangled information, which would necessitate several layers mance is strong: the correlation between the language perfor-
before being usable57–61.                             mance and brain scores is above R = 0.90 for MEG and R = 0.80
  Furthermore, the comparison between visual, lexical, and for fMRI (Supplementary Fig. 1). Nevertheless, it is clear that
compositional embeddings precise the nature and dynamics of improving the the signal-to-noise ratio, for instance by using
these cortical representations. In particular, our results shows increasingly large datasets20,29,47,72 will be critical to precisely
with unprecedented spatio-temporal precision that early visual characterize the nature of brain representations.
responses (<150 ms) are quasi-entirely accounted for by visual Permutation feature importance shows that several factors such
embeddings, and then transmitted to the posterior fusiform as the amount of training and the architecture signiﬁcantly
gyrus, which switches from visual to lexical representations impact brain scores. This ﬁnding contributes to a growing list of
around 200 ms (Movie 2). This ﬁnding strengthens the claim that variables that lead deep language models to behave more-or-less
this area is responsible for orthographic and morphemic similarly to the brain. For example, Hale et al.36 showed that the
computations39,62,63. Then, around 400 ms, word embeddings amount and the type of corpus impact the ability of deep lan-
predict a large fronto-temporo-parietal network which peaks in guage parsers to linearly correlate with EEG responses. The
the left temporal gyrus; these word representations are then present work complements this ﬁnding by evaluating the full set
maintained for several seconds17,19,31,33. This result not only of activations of deep language models. It further demonstrates
conﬁrms the wide spread distribution of meaning in the that the key ingredient to make a model more brain-like is, for
brain44,64, but also reveals its remarkably long-lasting nature. now, to improve its language performance.
  Finally, compositional embeddings peak in the brain regions The conclusion that deep networks converge towards brain-
associated with high-level language processing such as the infero- like representations should be qualiﬁed: we show that the brain
frontal and the anterior temporal cortices as well as the superior scores of the very best models tend to ultimately decrease with
temporal cortex and the temporal-parietal junction35,41,65.We language performance, especially in fMRI (Fig. 4g). We speculate
conﬁrm that these left-lateralized representations are signiﬁcant that this phenomenon (also observed in vision70) may rise
in both hemispheres66,67. Critically, MEG suggests that these because transformers overﬁt an inappropriate objective. Speciﬁ-
compositional effects become dominant and clearly bilateral long cally, while there is growing evidence that the human brain does
after word onset (>800 ms). We speculate that this surprisingly predict words from context30,73,74, this learning rule may not
late responses may be due to the complexity of the sentences used fully account for the complex (and potentially various) tasks
in the present study, which may slow down compositional performed by the brain (e.g., long-range75,76 and hierarchical
computations.                                         predictions77).
  At this stage, however, these three levels representations This discrepancy adds to the long-list of differences between
remain coarsely deﬁned. Further inspection of artiﬁcial8,68 and deep language models and the brain: whereas the brain is trained
biological networks10,28,69 remains necessary to further decom- (i) with a recurrent architecture and (ii) on a relatively small
pose them into interpretable features. In particular, it will be amount of grounded sentences, transformers are trained (i) with a
important to test whether the converging representations pre- massively feedforward architecture and (ii) on huge text
sently identiﬁed solely correspond to well-known linguistics databases7 (note that, given large-enough spaces, feedforward
phenomena as our supplementary analyses suggest (Supplemen- transformers may actually implement computations similar to
tary Fig. 2 and Supplementary Note 3), or, on the contrary, recurrent networks78). Consequently, while the similarity between
whether they correspond to unknown language structures. deep networks and the brain provide a stepping stone to unravel
  Second, our study shows that the similarity between deep the foundation of natural language processing, identifying the
language models and the brain primarily depends on their ability remaining differences between these two systems remains, by far,
to predict words from their context. Speciﬁcally, we show that the major challenge to build algorithms that learn and think like
language performance is the most contributing factor explaining humans7,9,79,80
the variability of brain scores across embeddings (Fig. 4d, h).
Analogous results have been reported in both vision and audition Methods
research, where best deep learning models tend to best map onto Deep language transformers. To model word and sentence representations, we
brain responses27,42,55,70,71. In addition, our results are consistent trained a variety of transformers4, and input them with the same sentences that the

6                  COMMUNICATIONS BIOLOGY |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1                                                          ARTICLE

subject read. Transformers consist of multiple contextual transformer layers contained a relative clause. Twenty percent of the sentences were followed by a yes/
stacked onto one non-contextualized word embedding layer (a look-up table). no question (e.g., “Did grandma give a cookie to the girl?”) to ensure that subjects
Following the standard implementation4,81,82, the word embedding layer is trained were paying attention. Questions were not included in the dataset, and thus
simultaneously with the contextual layers: the weights of the word embedding vary excluded from our analyses. Sentences were grouped into blocks of ﬁve sequences.
with the training, and so do their activations in response to ﬁxed inputs. Thus, one This grouping was used for cross-validation to avoid information leakage between
representation can be extracted from each (contextual or non-contextual) layer. We the train and test sets.
always extract the activations in a causal way: for example, given the sentence
“THE CAT IS ON THE MAT”, the brain response to “ON” would be solely
                                                                        Magnetic resonance imaging (MRI). Structural images were acquired with a T1-
compared to the activations of the transformer input with “THE CAT IS ON”, and
                                                                        weighted magnetization-prepared rapid gradient-echo (MP-RAGE) pulse
extracted from the “ON” contextualized embeddings. Word embeddings and
                                                                        sequence. The full acquisition details, available in ref. 37, are summarized here
contextualized embeddings were generated for every word, by generating word
                                                                        simplicity: TR = 2300 ms, TE = 3.03 ms, 8 degree ﬂip-angle, 1 slab, slice-matrix
sequences from the three previous sentences. We did not observe qualitatively size = 256 × 256, slice thickness = 1 mm, ﬁeld of view = 256 mm, isotropic voxel-
different results when using shorter or longer contexts. It is to be noted that the
                                                                        size = 1.0 × 1.0 × 1.0 mm. Structural images were defaced by Schoffelen and col-
sentences were isolated, and were not part of a narrative.
                                                                        leagues. Preprocessing of the structural MRI was performed with Freesurfer86,
  In total, we investigated 32 distinct architectures varying in their dimensionality
                                                                        using the recon-all pipeline and a manual inspection of the cortical segmen-
(∈ [128, 256, 512]), number of layers (∈ [4, 8, 12]), attention heads (∈ [4, 8]), and
                                                                        tations, realigned to “fsaverage”. Region-of-interest analyses were selected from the
training task (causal language modeling and masked language modeling). While
                                                                        PALS Brodmann’s Area atlas87 and the Destrieux atlas88.
causal language transformers are trained to predict a word from its previous                                   *
                                                                           Functional images were acquired with a T2 -weighted functional echo-planar
context, masked language transformers predict randomly masked words from a
                                                                        blood oxygenation level-dependent (EPI-BOLD) sequence. The full acquisition
surrounding context. We froze the networks at ≈100 training stages (log distributed
                                                                        details, available in ref. 37, are summarized here for simplicity: TR = 2.0 s,
between 0 and 4, 5 M gradient updates, which corresponds to ≈35 passes over the
                                                                        TE = 35 ms, ﬂip angle = 90 degrees, anisotropic voxel size = 3.5 × 3.5 × 3.0 mm
full corpus), resulting in 3600 networks in total, and 32,400 word representations
                                                        ’               extracted from 29 oblique slices. fMRI was preprocessed with fMRIPrep with
(one per layer). The training was early-stopped when the networks performance default parameters89. The resulting BOLD times series were detrended and de-
did not improve after ﬁve epochs on a validation set. Therefore, the number of
                                                                        confounded from 18 variables (the six estimated head-motion parameters
frozen steps varied between 96 and 103 depending on the training length.
                                                                        (trans   , rot   ) and the ﬁrst six noise components calculated using
  The algorithms were trained using XLM implementation6. No hyper-parameter    x,y,z  x,y,z
                                                                        anatomical CompCorr90 and six DCT-basis regressors using nilearn’s clean_img
tuning was performed. Following 6, each algorithm was trained on eight GPUs
                                                                        pipeline and otherwise default parameters91. The resulting volumetric data lying
using early stopping with training perplexity criteria, 16 streams per batch, 128
                                                                        along a 3 mm line orthogonal to the mid-thickness surface were linearly projected
words per stream, epoch size of 200,000 streams, 0.1 dropout, 0.1 attention
                                                                        to the corresponding vertices. The resulting surface projections were spatially
dropout, gelu activation, inverse (sqrt) adam optimizer with learning rate 0.0001,
                                                                        decimated by 10, and are hereafter referred to as voxels, for simplicity. Finally, each
0.01 weight decay, on the same Wikipedia corpus of 278,386,651 words (in Dutch) group of ﬁve sentences was separately and linearly detrended. It is noteworthy that
extracted using WikiExtractor83 and pre-processed using Moses tokenizer84, with
                                                                        our cross-validation never splits such groups of ﬁve consecutive sentences between
punctuation. We restricted the vocabulary to the 50,000 most frequent words,
                                                                        the train and test sets. Two subjects were excluded from the fMRI analyses because
concatenated with all words used in the study (50,341 vocabulary words in total).
                                                                        of difﬁculties in processing the metadata, resulting in 100 fMRI subjects.
These design choices enforce that the difference in brain scores observed across
models cannot be explained by differences in corpora and text preprocessing.
  To evaluate the language processing performance of the networks, we computed Magneto-encephalography (MEG). The MEG time series were preprocessed
their performance (top-1 accuracy on word prediction given the context) using a using MNE-Python and its default parameters except when speciﬁed92. Signals
test dataset of 180,883 words from Dutch Wikipedia. The list of architectures and were band-passed ﬁltered between 0.1 and 40 Hz ﬁltered, spatially corrected with a
their ﬁnal performance at next-word prerdiction is provided in Supplementary Maxwell Filter, clipped between the 0.01st and 99.99th percentiles, segmented
Table 2.                                                                between −500 ms to +2000 ms relative to word onset and baseline-corrected
  For clarity, we dissociate:                                           before t = 0. Reference channels and non-MEG channels were excluded from
                                                                        subsequent analyses, leading to 273 MEG channels per subject. We manually co-
  ●   The architectures (e.g., one transformer with 12 layers): there are 36 referenced (i) the skull segmentation of subjects’ anatomical MRI with (ii) the head
      transformer architectures here (18 CLM and 18 MLM).               markers digitized before MEG acquisition. A single-layer forward model was
  ●   The models: one architecture, frozen at one particular learning step. Since generated with the Freesurfer-wrapper implemented in MNE-Python92. Due to the
      we use 100 learning steps, there are 36 × 100 = 3600 networks here. lack of empty-room recordings, the noise covariance matrix used for the inverse
  ●   The embeddings: one word representation extracted from a network, at one operator was estimated from the zero-centered 200 ms of baseline MEG activity
      particular layer. Since the number of layers varies with the architecture preceding word onset. Subjects’ source space inverse operators were computed
      (twelve networks with 5, twelve networks with 9 and twelve networks with using a dSPRM. The average brain responses displayed in Fig. 1d were computed as
      13 twelve layers, including the non contextualized word embedding), there the square of the average evoked related ﬁeld across all words for each subject
      are 12 × (5 + 9 + 13) = 324 representations per step, so 324 × 100 = 3400 separately, averaged across subjects, and ﬁnally divided by their respective maxima,
      word embeddings in total.                                         to highlight temporal differences. Supplementary Movie 1 displays the average
                                                                        sources without normalization. Seven subjects were excluded from the MEG
                                                                        analyses because of difﬁculties in processing the metadata, resulting in 92 usable
Visual convolutional neural network. To model visual representations, every MEG recordings.
word presented to the subjects was rendered on a gray 100 × 32 pixel background
with a centered black Arial font, and input to a VGG network pretrained to Shared response model: Brain → Brain mapping. To estimate the amount of
recognize words from images85, resulting in an 888-dimensional embedding. explainable signal in each MEG and fMRI recording, we trained and evaluated,
Speciﬁcally, this model was trained on real pictures of single words taken in nat- through cross-validation, a linear mapping model W to predict the brain responses
uralistic settings (e.g., ad, banner).                                  of a given subject to each sentence Y from the aggregated brain responses of all
  This embedding was used to replicate and extend previous work on the  other subjects who read the same sentence X. Speciﬁcally, ﬁve cross-validation
similarity between visual neural network activations and brain responses to the splits were implemented across 5-sentence blocks with scikit-learn GroupKFold49.
same images (e.g., 42,52,53).                                           For each word of each sentence i, all but one subject who read the corresponding
                                                                                                                                           2
                                                                        sentence were averaged with one another to form a template brain response: xi
                                                                        Rn with n the number of MEG channels or fMRI voxels, as well as a target brain
Neuroimaging protocol. For all the analyses, we used the open-source dataset response y 2 Rn corresponding to the remaining subject. X and Y were normal-
                               37                                                i
released by Schoffelen and colleagues , gathering the functional magnetic reso- ized (mean = 0, std = 1) across sentences for each spatio-temporal dimension,
nance imaging (fMRI) and magneto-encephalography (MEG) recordings of 204 using a robust scaler clipping below and above the 0.01st and 99.99th percentiles,
                                                                                                          ´
native Dutch speakers (100 males), aged from 18 to 33 years. Here, we focused on respectively. A linear mapping W 2 Rn n was then ﬁt with a ridge regression to
the 102 right-handed speakers who performed a reading task while being recorded best predict Y from X on the train set:
by a CTF magneto-encephalography (MEG) and, in a separate session, with a
                                                                                                              À
SIEMENS Trio 3T Magnetic Resonance scanner37.                                                 ¼ð  T       þ λ Þ 1 T      ;                ð Þ
                                                                                           W     XtrainXtrain I XtrainYtrain               1
  Words (in Dutch) were ﬂashed one at a time with a mean duration of 351 ms
                                                                        with λ the l2 regularization parameter, chosen amongst 20 values log-spaced
(ranging from 300 to 1400 ms), separated with a 300 ms blank screen, and grouped −
into sequences of 9–15 words, for a total of approximately 2700 words per subject. between 10 3 and 108 with nested leave-one-out cross-validation for each
                                                                                                                                 ^
Sequences were separated by a 5 s-long blank screen. We restricted our study to dimension separately (as implemented in ref. 49). Brain predictions Y ¼ WX were
meaningful sentences (400 distinct sentences in total, 120 per subject). The exact evaluated with a Pearson correlation on the test set:
syntactic structures of sentences varied across all sentences. Roughly, sentences                  ¼     ð    ; ^ Þ:                      ð Þ
were either composed of a main clause and a simple subordinate clause, or                        R   Corr Ytest Ytest                      2


COMMUNICATIONS BIOLOGY   |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio                    7
ARTICLE                                                           COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1

  For the MEG source noise estimate, the correlation was also performed after Table 1 Brain parcellation Taxonomy used to label the
source projection:
                                                                         regions of interest in the brain following the PALS
                          ¼     ð     ; ^  Þ                      ð Þ    Brodmann’s Area atlas88.
                        R   Corr KYtest KYtest                     3

          n ´ m
with K 2 R   the inverse operator projecting the n MEG sensors onto m sources. Label                        Corresponding Brodmann’s areas
Correlation scores were ﬁnally averaged across cross-validation splits for each
subject, resulting in one correlation score (“brain score”) per voxel (or per MEG V1                        BA17
sensor/time sample) per subject.                                         Fusiform                           BA37
                                                                         Angular                            BA39
Brain score and similarity: Network → Brain mapping. To estimate the func- aSTG                             BA22-anterior
tional similarity between each artiﬁcial neural network and each brain, we followed mSTG                    BA22-middle
the same analytical pipeline used for noise ceiling, but replaced X with the acti- pSTG                     BA22-posterior
vations of the deep learning models. Speciﬁcally, using the same cross-validation, Supramarginal            BA40
                                                          ;
and for each subject separately, we trained a linear mapping W 2 Ro n with o the Infero-frontal             BA44/BA45/BA47
number of activations, to predict brain responses Y from the network activations X. Fronto-polar            BA10
                                 =      =
X was normalized across words (mean 0, std 1).                           Temporo-polar                      BA38
  To account for the hemodynamic delay between word onset and the BOLD
response recorded in fMRI, we used a ﬁnite impulse response (FIR) model with ﬁve
delays (from 2 to 10 s) to build X* from X. W was found using the same ridge
regression described above, and evaluated with the same correlation scoring Statistics and reproducibility. To estimate the robustness of our results, we
procedure. The resulting brain correlation scores measure the linear relationship systematically performed second-level analyses across subjects. Speciﬁcally, we
between the brain signals of one subject (measured either by MEG or fMRI) and applied Wilcoxon signed-rank tests across subjects’ estimates to evaluate whether
the activations of one artiﬁcial neural network (e.g., a word embedding). For MEG, the effect under consideration was systematically different from the chance level.
we simply ﬁt and evaluated the model activations X at each time sample  The p-values of individual voxel/source/time samples were corrected for multiple
independently.                                                          comparisons, using a False Discovery Rate (Benjamini/Hochberg) as implemented
  In principle, one may orthogonalize low-level representations (e.g., visual in MNE-Python92 (we use the default parameters). Error bars and ± refer to the
features) from high-level network models (e.g., language model), to separate the standard error of the mean (SEM) interval across subjects.
speciﬁc contribution of each type of model. This is because middle layers have
access to the word-embedding layer, and can, in principle, simply copy some of its Brain parcellation. In Fig. 3, we focus on particular regions of interest using the
activations. Similarly, word embedding can implicitly contain visual information: Brodmann’s areas from the PALS parcellation of freesurfer86. The superior tem-
e.g., frequent words tend to be visually smaller than rare ones. In our case, however, poral gyrus (BA22) is split into its anterior, middle and posterior parts to increase
the middle layers of transformers were much better than word embeddings, which granularity. For clarity, we rename certain areas as speciﬁed in Table 1.
were much better than visual embeddings. To quantify the gain ΔR achieved by a
higher-level model M1 (e.g., the middle layers of a transformer) and a lower level
                                                                        Ethics. These data were provided (in part) by the Donders Institute for Brain,
model M2 (e.g., a word embedding) we thus simply compared the difference of
their encoding scores:                                                  Cognition, and Behavior after having been approved by the local ethics committee
                                                                        (CMO—the local “Committee on Research Involving Human Subjects” in the
                                                                        Arnhem-Nijmegen region). As stated in the original paper37, “In the informed
                          ΔR   ¼ R   À R                          ð4Þ
                            M1    M1    M2                              consent procedure, [the subjects] explicitly consented for the anonymized collected
                                                                        data to be used for research purposes by other researchers. [..] The study was
Results are consistent when using different orthogonalization methods   approved by the local ethics committee (CMO—the local “Committee on Research
(Supplementary Fig. 5).                                                 Involving Human Subjects” in the Arnhem-Nijmegen region) and followed
                                                                        guidelines of the Helsinki declaration.”
Convergence analysis. All neural networks but the visual CNN were trained from
                                       ﬁ   “        ”
scratch on the same corpus (as detailed in the rst Methods section). We sys- Reporting summary. Further information on research design is available in the Nature
tematically computed the brain scores of their activations on each subject, sensor Research Reporting Summary linked to this article.
(and time sample in the case of MEG) independently. For computational reasons,
we restricted model comparison on MEG encoding scores to ten time samples
regularly distributed between [0, 2]s. Brain scores were then averaged across spatial Data availability
dimensions (i.e., MEG channels or fMRI surface voxels), time samples, and subjects The data are publicly available on request. They were provided by the Donders Institute
to obtain the results in Fig. 4. To evaluate the convergence of a model, we com- for Brain, Cognition and Behavior after having been approved by the local ethics
puted, for each subject separately, the correlation between (1) the average brain committee (CMO—the local “Committee on Research Involving Human Subjects” in the
score of each network and (2) its performance or its training step (Fig. 4 and Arnhem-Nijmegen region). Link: https://data.donders.ru.nl/collections/di/dccn/
Supplementary Fig. 1). Positive and negative correlations indicate convergence and DSC_3011020.09_236. The aggregated data used to generate Fig. 2 (Supplementary
divergence, respectively. Brain scores above 0 before training indicate a fortuitous Data 1), Fig. 4 (Supplementary Data 2), and Fig. 3 (Supplementary Data 3) are available
relationship between the activations of the brain and those of the networks. jointly with the manuscript. In particular, to generate Fig. 3, one needs Supplementary
                                                                        Data 3a (scores across layers for MEG, Fig. 3a), Supplementary Data 3b (scores across
Permutation feature importance. To systematically quantify how the archi- training for MEG, Figure Fig. 3b), Supplementary Data 3c (permutation importance for
tecture, language accuracy, and training of the language transformers impacted MEG, Fig. 3e), Supplementary Data 3d (scores across layers for fMRI, Fig. 3e),
their ability to linearly map onto brain activity, we ﬁtted, for each subject sepa- Supplementary Data 3e (scores across training for fMRI, Fig. 3f), and Supplementary
rately, a Random Forest across the models’ properties to predict their brain scores, Data 3f (permutation importance for fMRI, Fig. 3h).
using scikit-learn’s RandomForest48,49. Speciﬁcally, we input the following
                                                               “    ”
features to the random forest: the training task (causal language modeling CLM Code availability
vs. masked language modeling “MLM”), the number of attention heads ∈ [4, 8], the
                                                                        The code is available upon request. Data analysis was performed in Python using the
total number of layers ∈ [4, 8, 12], dimensionality ∈ [128, 256, 512], training step
                         ∈                                              scikit-learn open source library49. The MEG and fMRI data were processed using MNE-
(number of gradient updates, [0, 4.5M]), language modeling accuracy (top-1   92      91            86
accuracy at predicting a masked word) and the relative position of the repre- Python , nilearn and freesurfer . The natural language processing algorithms were
sentation (a.k.a “layer position”, between 0 for the word-embedding layer, and 1 for trained using the implementation from the XLM github repository (https://github.com/
                                                                                          6
the last layer). The performance of the Random Forest was evaluated for each facebookresearch/XLM, ).
subject separately with a Pearson correlation R using ﬁve-split cross-validation
across models.                                                          Received: 12 August 2021; Accepted: 29 December 2021;
  “Permutation feature importance” summarizes how each of the covarying
properties of the models (their task, architecture, etc.) speciﬁcally impacts the brain
scores48. Permutation feature importance was implemented with scikit-learn49 and
is summarized with ΔR: the decrease in R when shufﬂing one feature (using 50
repetitions). For each subject, we reported the average decrease across the cross-
validation splits (Fig. 4). The resulting scores (ΔR) are expected to be centered References
around 0 if the corresponding feature does not impact the brain scores, and 1. Turing, A. M. Parsing the Turing Test 23–65 (Springer, 2009).
positive otherwise.                                                     2.  Chomsky, N. Language and Mind (Cambridge University Press, 2006).

8                         COMMUNICATIONS BIOLOGY   |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio
COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1                                                          ARTICLE

3.  Dehaene, S., Yann, L. & Girardon, J. La plus belle histoire de l’intelligence: des 30. Goldstein, A. et al. Thinking ahead: prediction in context as a keystone of
    origines aux neurones artiﬁciels: vers une nouvelle étape de l’évolution (Robert language in humans and machines. Preprint at bioRxiv (2020).
    Laffont, 2018).                                                     31. Jain, S. & Huth, A. in Advances in Neural Information Processing Systems (eds
4.  Vaswani, A. et al. Attention is all you need. In Proceedings on NIPS (Cornell Bengio, S. et al.) vol. 31, 6628–6637 (Curran Associates, Inc., 2018).
    University, 2017).                                                  32. Athanasiou, N., Iosif, E. & Potamianos, A. Neural activation semantic models:
5.  Devlin, J., Chang, M., Lee, K. & Toutanova, K. BERT: pre-training of deep computational lexical semantic models of localized neural activations. In
    bidirectional transformers for language understanding. In Proceedings of the Proceedings of the 27th International Conference on Computational Linguistics
    2019 Conference of the North American Chapter of the Association for    2867–2878 (Association for Computational Linguistics, 2018).
    Computational Linguistics: Human Language Technologies, Volume 1 (Long 33. Toneva, M. & Wehbe, L. Interpreting and improving natural-language
    and Short Papers) (2019).                                               processing (in machines) with natural language-processing (in the brain).
6.  Lample, G. & Conneau, A. Cross-lingual language model pretraining. In Adv. Advances in Neural Information Processing Systems 32 (2019).
    Neural Inf. Process. Syst. (2019).                                  34. Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. & de Lange, F. P. A
7.  Brown, T. B. et al. Language models are few-shot learners. In Advances in hierarchy of linguistic predictions during natural language comprehension.
    Neural Information Processing Systems (2020).                           bioRxiv https://doi.org/10.1101/2020.12.03.410399 (2020).
8.  Lakretz, Y. et al. The emergence of number and syntax units in LSTM 35. Brennan, J. R. & Pylkkänen, L. Meg evidence for incremental sentence
    language models. In Proceedings of the 2019 Conference of the North American composition in the anterior temporal lobe. Cogn. Sci. 41, 1515–1531 (2017).
    Chapter of the Association for Computational Linguistics: Human Language 36. Hale, J., Dyer, C., Kuncoro, A. & Brennan, J. R. Finding syntax in human
    Technologies, Volume 1 (Long and Short Papers) (2019).                  encephalography with beam search. Preprint at https://arXiv.org/1806.04127
9.  Loula, J., Baroni, M. & Lake, B. M. Rearranging the Familiar: Testing   (2018).
    Compositional Generalization in Recurrent Networks. In BlackboxNLP@ 37. Schoffelen, J. -M. et al. A 204-subject multimodal neuroimaging dataset to
    EMNLP  (2018).                                                          study language processing. Sci. Data 6,1–13 (2019).
10. Hale, J. T. et al. Neuro-computational models of language processing. 38. Fedorenko, E., Blank, I., Siegelman, M. & Mineroff, Z. Lack of selectivity for
11. Lake, B. M. & Murphy, G. L. Word meaning in minds and machines. Psychol. syntax relative to word meanings throughout the language network. Cognition
    Rev. (2021).                                                            203, 104348 (2020).
12. Marcus, G. Deep learning: a critical appraisal. Preprint at https://arXiv.org/ 39. Dehaene, S. & Cohen, L. The unique role of the visual word form area in
    1801.00631 (2018).                                                      reading. Trends Cogn. Sci. 15, 254–262 (2011).
13. Bengio, Y., Ducharme, R. & Vincent, P. in Advances in Neural Information 40. Hagoort, P. The neurobiology of language beyond single-word processing.
    Processing Systems (eds. Leen, T. K. et al.) vol. 13, 932–938 (MIT Press, 2003). Science 366,55–58 (2019).
14. Mikolov, T., Chen, K., Corrado, G. & Dean, J. Efﬁcient estimation of word 41. Hickok, G. & Poeppel, D. The Cortical Organization of Speech Processing vol.
    representations in vector space. Preprint at https://arxiv.org/1301.3781 (2013). 8, 393–402 (Nature Publishing Group, 2007).
15. Pennington, J., Socher, R. & Manning, C. D. Glove: global vectors for word 42. Yamins, D. L. et al. Performance-optimized hierarchical models predict
    representation. In Empirical Methods in Natural Language Processing     neural responses in higher visual cortex. Proc. Natl Acad. Sci. 111, 8619–8624
    (EMNLP) Conference 1532–1543 (2014).                                    (2014).
16. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T. Enriching Word Vectors 43. Fedorenko, E. et al. Neural correlate of the construction of sentence meaning.
    with Subword Information. In Transactions of the Association for        Proc. Natl Acad. Sci. 113, E6256–E6262 (2016).
    Computational Linguistics (2016).                                   44. Huth, A. G., de Heer, W. A., Grifﬁths, T. L., Theunissen, F. E. & Gallant, J. L.
17. Mitchell, T. M. et al. Predicting human brain activity associated with the Natural speech reveals the semantic maps that tile human cerebral cortex.
    meanings of nouns. Science 320, 1191–1195 (2008).                       Nature 532, 453–458 (2016).
18. Anderson, A. J. et al. Multiple regions of a cortical network commonly encode 45. Seydell-Greenwald, A., Wang, X., Newport, E., Bi, Y. & Striem-Amit, E.
    the meaning of words in multiple grammatical positions of read sentences. Spoken language comprehension activates the primary visual cortex. Preprint
    Cereb. Cortex 29, 2396–2411 (2019).                                     at bioRxiv (2020).
19. Sassenhagen, J. & Fiebach, C. J. Traces of meaning itself: Encoding 46. Kell, A., Yamins, D., Shook, E., Norman-Haignere, S. & McDermott, J. A task-
    distributional word vectors in brain activity. Neurobiology of Language 1.1, optimized neural network replicates human auditory behavior, predicts brain
    54–76 (2020).                                                           responses, and reveals a cortical processing hierarchy. Neuron 98, 630–644
20. Caucheteux, C., Gramfort, A. & King, J.-R. GPT-2’s Activations Predict the (2018).
    Degree of Semantic Comprehension in the Human Brain (Cold Spring Harbor 47. Millet, J. & King, J.-R. Inductive biases, pretraining and ﬁne-tuning jointly
    Laboratory Section: New Results, 2021).                                 account for brain responses to speech. Preprint at https://arXiv.org/
21. Reddy Oota, S., Manwani, N. & Raju S, B. fMRI semantic category decoding 2103.01032 [cs, eess, q-bio] (2021).
    using linguistic encoding of word embeddings. In International Conference on 48. Breiman, L. Random forests. Mach. Learn. 45,5–32 (2001).
    Neural Information Processing (Springer, Cham, 2018).               49. Pedregosa, F. et al. Scikit-learn: machine learning in python. J. Mach. Learn.
22. Abnar, S., Ahmed, R., Mijnheer, M. & Zuidema, W. H. Experiential,       Res. 12, 2825–2830 (2011).
    distributional and dependency-based word embeddings have complementary 50. Tang, H. et al. Recurrent computations for visual pattern completion. Proc.
    roles in decoding brain activity. In Proceedings of the 8th Workshop on Natl Acad. Sci. 115, 8835–8840 (2018).
    Cognitive Modeling and Computational Linguistics (CMCL 2018), (2018). 51. Khaligh-Razavi, S.-M. & Kriegeskorte, N. Deep supervised, but not
23. Ruan, Y. -P., Ling, Z. -H. & Hu, Y. Exploring semantic representation in brain unsupervised, models may explain it cortical representation. PLoS Comput.
    activity using word embeddings. In Proceedings of the 2016 Conference on Biol. 10, e1003915 (2014).
    Empirical Methods in Natural Language Processing, 669–679 (Association for 52. Kriegeskorte, N. Deep neural networks: a new framework for modeling
    Computational Linguistics, 2016).                                       biological vision and brain information processing. Annu. Rev. Vis. Sci. 1,
24. Brodbeck, C., Hong, L. E. & Simon, J. Z. Rapid transformation from auditory to 417–446 (2015).
    linguistic representations of continuous speech. Curr. Biol. 28, 3976–3983 (2018). 53. Güçlü, U. & van Gerven, M. A. Deep neural networks reveal a gradient in the
25. Gauthier, J. & Ivanova, A. Does the brain represent words? an evaluation of complexity of neural representations across the ventral stream. J. Neurosci. 35,
    brain decoding studies of language understanding. Preprint at https://  10005–10014 (2015).
    arXiv.org/1806.00591 (2018).                                        54. Eickenberg, M., Gramfort, A., Varoquaux, G. & Thirion, B. Seeing it all:
26. Wehbe, L., Vaswani, A., Knight, K. & Mitchell, T. Aligning context-based convolutional network layers map the function of the human visual system.
    statistical models of language with brain activity during reading. In   NeuroImage 152, 184–194 (2017).
    Proceedings of the 2014 Conference on Empirical Methods in Natural Language 55. Yamins, D. L. & DiCarlo, J. J. Using goal-driven deep learning models to
    Processing (EMNLP) 233–243 (Association for Computational Linguistics,  understand sensory cortex. Nat. Neurosci. 19, 356 (2016).
    2014).                                                              56. Saxe, A., Nelli, S. & Summerﬁeld, C. If deep learning is the answer, what is the
27. Schrimpf, M. et al. The neural architecture of language: Integrative modeling question? Nat. Rev. Neurosci. 22,1–13 (2020).
    converges on predictive processing. In Proceedings of the National Academy of 57. Minsky, M. & Papert, S. Perceptrons: An Introduction to Computational
    Sciences (2021).                                                        Geometry. (MIT Press, 1969).
28. Caucheteux, C., Gramfort, A. & King, J.-R. Disentangling syntax and 58. Cadieu, C. F. et al. Deep neural networks rival the representation of primate it
    semantics in the brain with deep networks. ICML 2021-38th International cortex for core visual object recognition. PLoS Comput. Biol. 10, e1003963
    Conference on Machine Learning (2021).                                  (2014).
29. Caucheteux, C., Gramfort, A. & King, J.-R. Model-based analysis of brain 59. Kriegeskorte, N., Mur, M. & Bandettini, P. A. Representational similarity
    activity reveals the hierarchy of language in 305 subjects. In EMNLP 2021— analysis—connecting the branches of systems neuroscience. Front. Syst.
    Conference on Empirical Methods in Natural Language Processing (2021).  Neurosci. 2, 4 (2008).


COMMUNICATIONS BIOLOGY   |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio                    9
ARTICLE                                                           COMMUNICATIONS BIOLOGY | https://doi.org/10.1038/s42003-022-03036-1

60. King, J.-R. & Dehaene, S. Characterizing the dynamics of mental     88. Destrieux, C., Fischl, B., Dale, A. & Halgren, E. Automatic parcellation of
    representations: the temporal generalization method. Trends Cogn. Sci. 18, human cortical gyri and sulci using standard anatomical nomenclature.
    203–210 (2014).                                                         Neuroimage 53,1–15 (2010).
61. Cohen, U., Chung, S., Lee, D. D. & Sompolinsky, H. Separability and geometry 89. Esteban, O. et al. fmriprep: a robust preprocessing pipeline for functional mri.
    of object manifolds in deep neural networks. Nat. Commun. 11,1–13 (2020). Nat. Methods 16, 111–116 (2019).
62. Hermes, D. et al. Electrophysiological responses in the ventral temporal cortex 90. Behzadi, Y., Restom, K., Liau, J. & Liu, T. T. A component based noise
    during reading of numerals and calculation. Cereb. Cortex 27, 567–575 (2017). correction method (compcor) for bold and perfusion based fmri. Neuroimage
63. Woolnough, O. et al. Spatiotemporal dynamics of orthographic and lexical 37,90–101 (2007).
    processing in the ventral visual pathway. Nat. Hum. Behav. 5, 389–398 (2021). 91. Abraham, A. et al. Machine learning for neuroimaging with scikit-learn. Front.
64. Price, C. J. The anatomy of language: a review of 100 fmri studies published in Neuroinform. 8, 14 (2014).
    2009. Ann. N. Y. Acad. Sci. 1191,62–88 (2010).                      92. Gramfort, A. et al. Mne software for processing meg and eeg data. NeuroImage
65. Pallier, C., Devauchelle, A.-D. & Dehaene, S. Cortical representation of the 86, 446–460 (2014).
    constituent structure of sentences. Proc. Natl Acad. Sci. 108, 2522–2527 93. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. Distributed
    (2011).                                                                 Representations of Words and Phrases and their Compositionality. Advances
66. Fedorenko, E., Hsieh, P.-J., Nieto-Castañón, A., Whitﬁeld-Gabrieli, S. & in Neural Information Processing Systems 3111–3119 (MIT Press, 2013).
    Kanwisher, N. New method for fmri investigations of language: deﬁning rois 94. Bingham, E. & Mannila, H. Random projection in dimensionality reduction:
    functionally in individual subjects. J. Neurophysiol. 104, 1177–1194 (2010). applications to image and text data. In Proceedings of the Seventh ACM
67. Cogan, G. B. et al. Sensory–motor transformations for speech occur bilaterally. SIGKDD International Conference on Knowledge Discovery and Data Mining
    Nature 507,94–98 (2014).                                                245–250 (ACM, 2001).
68. Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U. & Levy, O. Emergent 95. Frankle, J. & Carbin, M. The lottery ticket hypothesis: ﬁnding sparse, trainable
    linguistic structure in artiﬁcial neural networks trained by self-supervision. neural networks. arXiv preprint arXiv:1803.03635 (2018).
    Proc. Natl Acad. Sci. 117, 30046–30054 (2020).
69. Reddy, A. J. & Wehbe, L. Syntactic representations in the human brain:
    beyond effort-based metrics. Preprint at bioRXiv (2021).            Acknowledgements
70. Schrimpf, M. et al. Brain-score: which artiﬁcial neural network for object This work was supported by ANR-17-EURE-0017, the Fyssen Foundation, and the
    recognition is most brain-like? Preprint at bioRXiv (2018).         Bettencourt and Fyssen Foundations to J.R.K. for his work at PSL.
71. Kell, A. J. E., Yamins, D. L. K., Shook, E. N., Norman-Haignere, S. V. &
    McDermott, J. H. A task-optimized neural network replicates human auditory Author contributions
    behavior, predicts brain responses, and reveals a cortical processing hierarchy.
                                                                        J.R.K. deﬁned the line of research, C.C. conducted the analyses, both authors analyzed the
    Neuron 98, 630–644 (2018).
                                                                        results, designed the ﬁgures and wrote the paper.
72. Nastase, S. A. et al. Narratives: fmri data for evaluating models of naturalistic
    language comprehension. Trends in neurosciences 43, 271–273 (2020).
73. Keller, G. B. & Mrsic-Flogel, T. D. Predictive processing: a canonical cortical Competing interests
    computation. Neuron 100, 424–435 (2018).                            The authors declare no competing interests.
74. Heilbron, M., Armeni, K., Schoffelen, J.-M., Hagoort, P. & de Lange, F. P. A
    hierarchy of linguistic predictions during natural language comprehension. Additional information
    Preprint at bioRXiv (2020).
                                                                        Supplementary information
75. Wang, L. Dynamic predictive coding across the left fronto-temporal language                The online version contains supplementary material
    hierarchy: evidence from MEG, EEG and fMRI29.                       available at https://doi.org/10.1038/s42003-022-03036-1.
76. Lee, C. S., Aly, M. & Baldassano, C. Anticipation of temporally structured
                                                                        Correspondence
    events in the brain. eLife 10, e64972 (2021).                                    and requests for materials should be addressed to Charlotte Caucheteux
77. Friston, K. The free-energy principle: a uniﬁed brain theory? Nat. Rev. or Jean-Rémi King.
    Neurosci. 11, 127–138 (2010).
                                                                        Peer review information
78. Ramsauer, H. et al. Hopﬁeld networks is all you need. Preprint at https://              Communications Biology thanks Blake Richards, Anna Ivanova
    arXiv.org/2008.02217 [cs, stat] (2021).                             and the other, anonymous, reviewer for their contribution to the peer review of this
79. Lake, B. M., Ullman, T. D., Tenenbaum, J. B. & Gershman, S. J. Building work. Primary Handling Editors: Enzo Tagliazucchi and George Inglis. Peer reviewer
    machines that learn and think like people. Behavioral and brain sciences 40 reports are available.
    (2017).
                                                                        Reprints and permission information
80. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. & Choi, Y. Hellaswag: can a                       is available at http://www.nature.com/reprints
                 ﬁ
    machine really nish your sentence? Proceedings of the 57th Annual Meeting Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in
    of the Association for Computational Linguistics (2019).            published maps and institutional afﬁliations.
81. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep
    bidirectional transformers for language understanding. In Proceedings of
    NAACL-HLT   (2019).
82. Radford, A. et al. Language models are unsupervised multitask learners.          Open Access This article is licensed under a Creative Commons
    OpenAI Blog 1, 9 (2019).                                                         Attribution 4.0 International License, which permits use, sharing,
83. Attardi, G. Wikiextractor. https://github.com/attardi/wikiextractor (2015). adaptation, distribution and reproduction in any medium or format, as long as you give
84. Koehn, P. et al. Moses: ppen source toolkit for statistical machine translation. appropriate credit to the original author(s) and the source, provide a link to the Creative
    In Proceedings of the 45th Annual Meeting of the Association for    Commons license, and indicate if changes were made. The images or other third party
    Computational Linguistics Companion Volume Proceedings of the Demo and material in this article are included in the article’s Creative Commons license, unless
    Poster Sessions 177–180 (Association for Computational Linguistics, 2007). indicated otherwise in a credit line to the material. If material is not included in the
85. Baek, J. et al. What is wrong with scene text recognition model comparisons? article’s Creative Commons license and your intended use is not permitted by statutory
    dataset and model analysis. In Proceedings of the IEEE International regulation or exceeds the permitted use, you will need to obtain permission directly from
    Conference on Computer Vision, 4715–4723 https://github.com/clovaai/deep- the copyright holder. To view a copy of this license, visit http://creativecommons.org/
    text-recognition-benchmark (2019).                                  licenses/by/4.0/.
86. Fischl, B. Freesurfer. Neuroimage 62, 774–781 (2012).
87. Van Essen, D. C. A population-average, landmark-and surface-based (pals)
    atlas of human cerebral cortex. Neuroimage 28, 635–662 (2005).      © The Author(s) 2022, corrected publication 2023


10                        COMMUNICATIONS BIOLOGY   |          (2022) 5:134  | https://doi.org/10.1038/s42003-022-03036-1 | www.nature.com/commsbio