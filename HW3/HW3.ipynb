{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 資料視覺化網站實作2\n",
        "\n",
        "## Text Mining\n",
        "\n",
        "> LDA Model：  \n",
        "Latent Dirichelet Allocation，用以分析字詞的出現頻率，可以進行對文章的主題分類。\n",
        "\n",
        "\n",
        "\n",
        "> Corpora：  \n",
        "gensim的class，可以創建提供給LDA Model的Corpus(詞袋)  \n",
        "可以利用GPT-3.5 API生成文件summary再進行分析。\n",
        "\n",
        "\n",
        "\n",
        "> 文字處理時建議先進行Preprocessing排除部分泛用字眼(the, and, etc)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fVHt_CULQWVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#upload key file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Zt7u0tCqdH6E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "c2840236-8745-40b2-d3c0-6a6fbf18ebae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6286726d-756f-49e9-ab5a-03b5c5b466c2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6286726d-756f-49e9-ab5a-03b5c5b466c2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving key.txt to key (1).txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#upload key file(Must named \"key.txt\")\n",
        "keyfile = open(\"/content/key.txt\",'r')\n",
        "TOKEN = keyfile.readline()\n",
        "\n",
        "openai.api_key = TOKEN"
      ],
      "metadata": {
        "id": "kjJT9dXXdz0m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload paper file(Must named \"01.txt\",\"02.txt\",\"03.txt\")\n",
        "fileSize = 8 #Define num of file\n",
        "%cd \"/content/paper\"\n",
        "for i in range(fileSize):\n",
        "  upload = files.upload()"
      ],
      "metadata": {
        "id": "_Yc3YjqItl5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "以上程式若已經上傳檔案可以跳過"
      ],
      "metadata": {
        "id": "XMMARIjc1Rd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install pyLDAvis\n",
        "!pip install stemming"
      ],
      "metadata": {
        "id": "NXB_toRbbqg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9f6c75-12bd-4291-8032-9f0a229fdeee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.9/dist-packages (0.27.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.9/dist-packages (3.4.0)\n",
            "Requirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.5.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (2.8.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.22.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (4.3.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.10.1)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (67.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyLDAvis) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyLDAvis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stemming in /usr/local/lib/python3.9/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rj9PxC1HQSwD"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import openai\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#readfile\n",
        "itemlists = []\n",
        "for i in range(1,fileSize+1):\n",
        "  with open('/content/paper/0'+str(i)+'.txt', 'r') as fh:\n",
        "    tmp = fh.read()\n",
        "    itemlists.append(tmp.split(','))"
      ],
      "metadata": {
        "id": "5zbme7T7efOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data input function\n",
        "def chatgptfn(sub_list):\n",
        "    result = ''\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{sub_list} :give me a summary\"}\n",
        "        ]\n",
        "    )\n",
        "    for choice in response.choices:\n",
        "        result += choice.message.content\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "-eeoNQaIcBgz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create corpus\n",
        "import time\n",
        "datas = []\n",
        "for itemlist in itemlists:\n",
        "  length = int(len(itemlist)/5)\n",
        "  datas.append([itemlist[0:length], itemlist[length+1:length*2], \n",
        "          itemlist[length*2+1:length*3], itemlist[length*3+1:length*4], itemlist[length*4+1:-1]])\n",
        "for data in datas:\n",
        "  for i in range(5):\n",
        "    data[i] = chatgptfn(data[i])\n",
        "    print(str(i+1),', ',data[i])\n"
      ],
      "metadata": {
        "id": "1gdw7WkKcbJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "至此已將論文進行分段summary，接下來進行預處理：\n",
        "\n",
        "* Removing stop words: filter out commonly used and auxiliary words (e.g., “the”, “a”, “an”, “in,” “she”)\n",
        "\n",
        "* Removing irrelevant characters: ignore numbers, punctuation, symbols, etc.\n",
        "\n",
        "* Tokenization: split a sequence of strings into tokens (e.g., words, keywords, phrases, sentences, and other elements); enables analysis down to the level of segmentation; used in the models, like bag-of-words, for term frequency counting, text clustering, and document matching tasks\n",
        "\n",
        "* Stemming: reduce inflected words to their root forms (e.g., trouble, troubled, troubling → troubl-); improves the performance of text clustering tasks by reducing dimensions (i.e., the number of terms to be processed)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_C0LBMSslb5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing texts\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from stemming.porter2 import stem\n",
        "import string\n",
        "texts = []\n",
        "for data in datas:\n",
        "  print(data)\n",
        "  texts.append([stem(word) for word in remove_stopwords(document.lower()).split()] for document in data)\n",
        "texts = [i for text in texts for i in text]\n",
        "dictionary = gensim.corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "id": "NkIKZLTBmi4g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45a42414-4ba2-4d01-fad9-5034c3aac3ca"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The paper discusses the creation of a new model called GloVe (Global Vectors for Word Representation) for word representations. It analyzes the properties necessary for creating linear meanings in word vectors and argues that global log-bilinear regression models are appropriate for doing so. The paper then proposes a specific weighted least squares model that trains on global word-word co-occurrence counts, making efficient use of statistics while generating a vector space with meaningful substructure. GloVe outperforms other current models on similarity tasks, named entity recognition, and several word analogy tasks.', 'The text discusses a model for learning word vectors that is based on ratios of co-occurrence probabilities. The goal is to distinguish between relevant and irrelevant words in a corpus. The model uses a weighting function in the cost function to give more weight to frequent co-occurrences and less weight to rare ones. The model is compared to other models like skip-gram and ivLBL and is found to perform well, especially when the weighting function is chosen appropriately.', 'The text discusses a model called GloVe (Global Vectors for Word Representation) which learns word representations based on co-occurrence statistics. The model is evaluated on several tasks including the word analogy task, word similarity tasks, and named entity recognition. The authors also explore the impact of different choices for context window size and whether to distinguish between left and right context. Results show that GloVe outperforms other models on many tasks and has better computational efficiency.', \"The article discusses the development of the GloVe model, a method for generating vector representations of words that can be used in natural language processing tasks. The article describes the training process for the model and evaluates its performance on various tasks. It also analyzes the effects of varying vector length, context size, and corpus size on the model's performance. Additionally, the article compares GloVe to other methods for generating word vectors, including word2vec, and discusses the feasibility of training the model on large datasets. Overall, the GloVe model performs well on various natural language processing tasks and is an effective method for generating word vectors.\", 'The text discusses various methods and models for unsupervised learning of word representations that capture meaningful linear substructures prevalent in recent log-bilinear prediction-based methods like word2vec. The GloVe model, a new global log-bilinear regression model, is introduced and demonstrated to outperform other models on tasks such as word analogy, word similarity, and named entity recognition. The paper acknowledges that count-based and prediction-based methods are not dramatically different at a fundamental level, and a model that utilizes the main benefit of count data while simultaneously capturing the meaningful linear substructures prevalent in recent prediction-based methods is proposed. The authors thank the anonymous reviewers for their valuable comments and acknowledge the support of various organizations.']\n",
            "['The article discusses Natural Language Processing (NLP), which is a computerized approach to analyzing text based on a set of theories and technologies. NLP involves analyzing naturally occurring texts in different languages and modes for the purpose of achieving human-like language processing for a range of tasks or applications. NLP has made serious inroads into accomplishing goals such as translating text into another language and answering questions about the contents of text. However, the goal of NLP is true Natural Language Understanding (NLU), which has not yet been accomplished. The article highlights the origins of NLP, the different focuses of language processing and generation, and early work in machine translation and speech recognition.', 'The article discusses the history of natural language processing (NLP) and its levels of processing. The article notes that in the 1950s, there was over-enthusiasm in the field, but by the 1960s, there were significant developments in prototype systems and theoretical work. The focus in the late 1960s and early 1970s was on how to represent meaning and developing computationally tractable solutions. The article notes that in the last ten years of the millennium, the field was growing rapidly, and researchers were developing next-generation NLP systems that deal reasonably well with general text. The article also explains the levels of language processing within NLP, including phonology, morphology, and lexical analysis.', 'The article discusses the different levels of natural language processing (lexical, syntactic, semantic, discourse, and pragmatic) and how they contribute to understanding language. It also explores the different approaches to natural language processing, including symbolic and statistical methods. While current NLP systems tend to focus on the lower levels of processing, there is ongoing research and development in incorporating the higher levels.', 'The field of natural language processing (NLP) has seen the coexistence of symbolic and statistical approaches since its early days. Symbolic approaches involve deep analysis of linguistic phenomena through human-developed rules and lexicons, while statistical approaches use observable data to develop generalized models of linguistic phenomena. Connectionist approaches combine statistical learning with various theories of representation and develop generalized models from examples of linguistic phenomena. Each approach follows a general set of steps, including data collection, analysis/model building, rule/data construction, and application. Each approach has its strengths and weaknesses, making them more or less suitable for various NLP tasks.', 'The article discusses three types of approaches in Natural Language Processing (NLP) - symbolic, statistical, and connectionist. Symbolic models use human introspective data, statistical models use machine-observable facets of data, and connectionist models store knowledge across a network. Each approach has its own characteristics and is suitable for different tasks. The article also provides examples of NLP applications, including information retrieval, information extraction, question-answering, summarization, machine translation, and dialogue systems. Hybrid techniques that utilize the strengths of each approach are being developed to address NLP problems more effectively.']\n",
            "['The article describes a new optimization algorithm, QGDECC, which is designed to minimize train delays in railway networks using a combination of cooperative co-evolutionary differential evolution and quantum evolutionary and genetic algorithms. The algorithm incorporates multiple strategies to improve accuracy and performance and outperforms other benchmark algorithms in reducing the impact of delays on the network. The article also provides a summary of the fundamental algorithms used to develop QGDECC.', 'This paper proposes a new optimization algorithm called QGDECC, which combines quantum-inspired evolutionary algorithms with adaptive cooperative co-evolutionary algorithms to solve complex optimization problems, specifically in railway network scheduling. The algorithm uses a quantum variable decomposition strategy to optimize variables that are interdependent, and incorporates a parameter adaptive strategy and an increment mutation method to accelerate convergence speed. The paper provides a detailed description of the proposed algorithm and its framework.', 'The given text describes a train scheduling problem, where the objective is to minimize the gap between the rescheduled train schedule and original train schedule, while meeting various safety and operation constraints. A train delay scheduling method based on QGDECC is proposed as a solution for this problem, which adjusts train speed and order in the railway network when delays occur, providing a balance between convergence speed and accuracy. The proposed method aims to better optimize the results for large-scale train scheduling problems.', 'The article presents various models and algorithms for scheduling optimization in different scenarios, such as vehicle-to-grid, train operation adjustment, job shop scheduling, passenger flow control, and subway train scheduling, among others. The authors propose combining multiple optimization objectives to better serve the interests of different stakeholders. The article highlights the importance of considering complex and time-dependent factors in scheduling optimization and mentions several algorithms, including a hybrid differential evolution algorithm and a meta-knowledge transfer-based differential evolution, for global optimization. The authors declare no competing financial interests or personal relationships that may have influenced the reported work. The research is supported by the National Natural Science Foundation.', 'The list consists of references to several papers on optimization algorithms and their applications in different fields such as railway traffic rescheduling, global optimization problems, parameter identification of lithium-ion batteries, train speed trajectory optimization, delay recovery in high-speed rail, and dynamic multiobjective optimization. The papers discuss different approaches including improved differential evolution algorithm, MSIQDE algorithm with multiple strategies, cooperative co-evolutionary differential evolution algorithm, adaptive differential evolution algorithm, recursive differential grouping, and adaptive competitive swarm optimizer.']\n",
            "['This paper discusses the role of control theory and engineering in solving societal challenges and provides a roadmap for future education and research in this area. The authors aim to disseminate and interpret the findings of an IEEE Control Systems Society working group and promote discussion within the community. The paper also emphasizes the importance of reimagining control-related topics in education, particularly at the tertiary level and beyond.', 'The passage discusses the relevance of feedback and control in modern biological systems and the need to provide tools for biological scientists and engineers to be successful. It also mentions the challenges in organizing resources on a website and providing a search engine. The passage suggests categorizing resources by topic, level, and format, and mentions a survey that collected freely available resources on the first control course. The passage also discusses the importance of teaching a single course accessible to all students or developing specialized courses for specific audiences. Finally, the passage emphasizes the importance of a control repository for diversity and inclusion and as a social elevator for students with low incomes.', \"The article discusses best practices in engineering education and shares specific examples from UC Berkeley's EECS program. The program offers diverse perspectives, hands-on learning experiences, and supportive resources to help students succeed in challenging material. Course titles should reflect relevance and excitement, and learning resources should be modularized for ease of sharing and adoption. Students also have opportunities for extra credit through art and media projects related to the course. The article includes illustrations of student projects and learning labs.\", 'The given text describes a test-bed for co-planar multi-rotor unmanned aerial vehicles (UAVs) to be used for educational purposes. The system is underactuated, with longitudinal motion control only indirectly influencing translational motion. The test-bed comprises a functional quadrotor, an aluminum frame with steel bars, and a ground control station. Two operational modes exist: attitude mode (1 degree-of-freedom) and position mode (2 degrees-of-freedom). The text also mentions a loop-shaping approach for tuning the proportional, integral, and derivative (PID) gains of the controller for satisfactory robustness and performance.  Overall, this text provides an overview of the test-bed setup and its various components.', 'The given sources relate to various aspects of control engineering education, including remote and traditional laboratory methods, crop growth control, economic model predictive control, and online experimental frameworks. The sources also cover topics such as MATLAB assessment, control curricula, and feedback in control systems. Some of the sources provide insights into the use of interactive tools and software for teaching/assessment. Additionally, some materials focus on the technical committee on control education and report on future directions in control, dynamics, and systems. Overall, the sources can provide valuable information to educators and researchers interested in the field of control engineering education.']\n",
            "['The article is a systematic review of empirical research from 2011 to 2020 on the application of artificial intelligence (AI) in online higher education. The study examines the functions of AI, algorithms used, and effects and implications generated by empirical research. The review found that the functions of AI applications in online higher education include prediction of learning status, performance, recommendation, automatic assessment, and improvement of learning experience. Traditional AI technologies are commonly adopted while more advanced techniques are rarely used yet. Effects generated by AI applications include high-quality predictions and recommendations, improved academic performance, and improved online engagement and participation. The article proposes theoretical, technological, and practical implications for the integration of educational and learning theories into AI-enabled online learning.', 'The article reviews 32 different studies that apply AI in online education. The authors categorize the functions of AI in education into four types: personalized support, automatic content recommendation, automatic assessment, and improvement of the learning experience. Among these functions, personalized support (50%) and automatic content recommendation (38%) are the most commonly studied. The authors also identify the most commonly used AI algorithms, including DT, NN, and NB. Overall, the studies suggest that AI has the potential to improve the quality of online education by providing personalized support and recommendations, assessing student performance, and enhancing the learning experience.', 'This article is a systematic review of the applications of AI in online higher education. The article discusses the importance of providing a bridge between educators or students and AI systems or tools to help obtain a multifaceted understanding of student status and achieve good learning performance predictions. The article also addresses the need for more empirical research to examine the different roles of AI in online higher education, how AI is connected to existing educational and learning theories, and to what extent the use of AI technologies influence online learning quality. The review points out a discrepancy between the potential of AIEd and their actual implementations in online higher education caused by a separation of AI technology and the complex educational system. The article suggests the need to design and apply AIEd with awareness that the AI technology is a part of a larger educational system. Overall, the article emphasizes the need for more empirical research to examine the roles of AI in online higher education, educational and learning theories underpinned AIEd, and the actual effects of AI on online learning quality.', 'The article provides a list of references related to the use of data mining, artificial intelligence, and personalized learning in education. These references include research on the use of recommendation algorithms, intelligent web-based learning systems, and evolutionary approaches for personalizing content delivery. The articles also explore the effectiveness of educational data mining for predicting academic failure and the use of automated feedback systems for learners. Overall, the articles highlight the potential of these technologies to enhance student learning and prevent academic drop-out.', 'This text is a list of academic papers related to various topics in education technology. The papers cover subjects such as personalized learning, predictive analysis tools in higher education, dropout prediction in e-learning courses, and more. The list includes details such as the title, authors, and publication information for each paper, as well as links to academic journals where they can be accessed. Additionally, the text includes a reference to the PRISMA statement, which sets guidelines for reporting systematic reviews and meta-analyses in research.']\n",
            "['The article discusses a study that compares deep learning algorithms used in natural language processing to the human brain. The study indicates that both the algorithms and the human brain rely on predicting words from context to process language. Additionally, the study found that the algorithms generate brain-like representations of language because of their ability to predict words and maintain perceptual, lexical, and compositional representations within each cortical region. The research provides insight into the computational principles of language processing in the human brain and offers a promising path to unraveling the foundations of natural language processing.', 'This passage discusses a study on the relationship between language performance and brain activity in language processing. The study used language transformers, specifically looking at the correlation between the activations of the transformers and brain activity measured by MEG and fMRI. The results showed an inverted U-shaped pattern across layers for all architectures and that middle layers outperformed input and output layers. Additionally, the best language transformers did not achieve the highest brain scores, and language performance was found to be the most important factor in explaining the variability of brain scores across models. The study suggests that better language models are those that more accurately predict words from context, and their activations linearly map onto those of the brain, particularly in the superior temporal sulcus and gyrus.', 'This text describes a study that evaluates the language processing performance of different neural network architectures on a Dutch language test dataset. The researchers computed Magneto-encephalography (MEG) to evaluate the performance of the networks. They used various design choices, such as restricting vocabulary to the 50,000 most frequent words and preprocessing using Moses tokenizer, to ensure that the difference in brain scores observed across models cannot be explained by differences in corpora and text preprocessing. They used various embeddings and models, resulting in a total of 3600 networks, to evaluate the performance of the networks at next-word prediction. The study took place on 100 fMRI subjects.', 'The given text is a list of references for various studies related to natural language processing, language models, and neuro-computational models of language processing. These studies include topics such as interpreting and improving natural-language processing, cross-lingual language model pretraining, emergences of number and syntax units, finding syntax in human encephalography, compositional generalization in recurrent networks, neuroimaging datasets to study language processing, unique role of the visual word form area in reading, and the cortical organization of speech processing among others.', 'This is a list of references cited in an article or study, discussing a range of topics related to the neural architecture of language, computational geometry, visual recognition, and machine learning in neuroimaging. Some specific topics mentioned include predictive processing, disentangling syntax and semantics in the brain, characterizing mental representations, and using functional MRI for investigations of language. Additionally, there are several references discussing specific methods and software used in neuroimaging research.']\n",
            "['This article discusses a new paradigm in natural language processing called \"prompt-based learning.\" Unlike traditional supervised learning, which trains a model to take in an input and predict an output, prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input is modified using a template into a textual string prompt that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string from which the final output can be derived. The article introduces the basics of this promising paradigm, describes a unified set of mathematical notations that covers a wide variety of existing work, and organizes existing work along several dimensions. The article also provides resources, such as a website and a survey of existing work, to make the field more accessible to interested beginners.', \"The article discusses the concept of prompt engineering in natural language processing, which aims to improve the performance of pre-trained language models by providing specific guidance to the model during task solving. Prompt engineering includes two main components: prompt template engineering, which designs appropriate inputs for prompting methods, and prompt answer engineering, which involves searching for an effective answer space and map for the model's output. The article discusses different approaches to designing prompt templates and answer spaces, including manual design and using machine learning techniques. The shape of the answer is an important consideration and can be tokens, a span of text, or a sentence or document, depending on the task requirements. The effectiveness of prompt engineering has been demonstrated in various NLP tasks such as sentiment classification, entity recognition, and question answering.\", 'The article being referenced is \"A Systematic Survey of Prompting Methods in Natural Language Processing\" published in January 2023 in Volume 55, Issue 9. Table 5 in the article is discussing other research topics relevant to prompting methods, such as prompt ensembling, few-shot learning, and larger-context learning. Discrete prompt query search reformulation and prompt-based attention are also mentioned. The article explores different methods used in natural language processing and their effectiveness in improving downstream tasks. The use of prompt templates in prompt ensembling is highlighted as an efficient and effective way to generate multiple results without requiring multiple training times. Few-shot learning aims to gain generalized knowledge using only a few examples, while augmentation introduces additional training samples in an explicit way.', 'The article provides a survey of recent advancements in natural language processing (NLP), focusing on deep learning techniques and pre-trained language models such as BERT, GPT, and XLNet. The authors discuss various NLP tasks, including summarization, machine translation, named entity recognition, and sentiment analysis, and highlight the benefits and limitations of different approaches. They also discuss challenges and future directions in NLP research, such as developing more efficient and interpretable models, addressing bias and ethical concerns, and improving data augmentation techniques for low-resource languages.', 'The article is a survey of recent advances in natural language processing (NLP) that use deep learning techniques. It covers a wide range of topics, such as language modeling, sentiment analysis, language translation, and question answering. The authors review recent research on pre-trained language models, such as BERT, GPT, and RoBERTa, which have achieved impressive results on a variety of NLP tasks. They also discuss the use of knowledge graphs and common sense reasoning in NLP, and explore the limitations of current NLP models. Overall, the survey provides a comprehensive overview of the current state of the art in the field of NLP, and highlights future directions for research.']\n",
            "[\"This paper introduces a new metaheuristic optimization algorithm called War Strategy Optimization (WSO), which is based on the strategic movement of army troops during war. The algorithm models two popular war strategies, attack and defense, and updates the positions of soldiers on the battlefield in accordance with the strategy implemented. A novel weight updating mechanism and a weak soldier's relocation strategy are introduced to improve the algorithm's convergence and robustness. The efficacy of the proposed algorithm is tested on 50 benchmark functions and four engineering problems and compared with ten popular metaheuristic algorithms, demonstrating its superiority.\", \"The proposed algorithm, WSO, is analyzed for its performance under various conditions to highlight its salient features. The algorithm involves locating the positions of the king and commander, distributing soldiers randomly, and obtaining attack forces. The soldiers are then updated and sorted based on their fitness using a combination of exploitation and exploration strategies. The algorithm's unique feature is the replacement/relocation of weak soldiers, which is examined through three cases. Adaptive weight mechanism assigned to each soldier is also examined through multiple operating cases. The algorithm performs well in multimodal functions and dominates other metaheuristic algorithms in the case of unimodal functions.\", 'This text provides a list of various metaheuristic optimization algorithms and their applications. The algorithms include Grasshopper Optimization Algorithm, Artificial Bee Colony Algorithm, Firefly Algorithm, Sine Cosine Algorithm, Social Spider Optimization Algorithm, Marine Predators Algorithm, and Krill Herd Algorithm, among others. The applications range from global optimization and engineering to solving constrained optimization problems. The article highlights the various strengths of each algorithm and their effectiveness in different scenarios.', 'The document contains a list of various metaheuristic algorithms in the field of global optimization. The algorithms mentioned include Emperor Penguin Optimizer, Spotted Hyena Optimizer, Coyote Optimization Algorithm, and Firefly Algorithm, among others. Each algorithm is briefly described with its applications and advantages. The paper serves as a good reference for researchers and practitioners interested in the field of optimization.', 'This is a list of various optimization algorithms used in computational and engineering applications. Some of the algorithms mentioned include swarm intelligence algorithms like Weighted Superposition Attraction (WSA), Modiﬁed Sine Cosine Algorithm, and Tribe-Charged System Search. Additionally, the list includes evolutionary algorithms like Differential Evolution Algorithm and Genetic Algorithms. Other algorithms mentioned are Water Wave Optimization and Lightning Search Algorithm. The applications of these algorithms vary from function optimization and engineering design to power systems and dynamic parameter adaptation.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create lda model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, \n",
        "                    num_topics=3, random_state=100, update_every=1, \n",
        "                    chunksize=100, passes=10, alpha='auto', per_word_topics=True)"
      ],
      "metadata": {
        "id": "ur8y99i1beh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a8de7cf-5fcc-4804-cbc9-cb661aeef4af"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classify answer\n",
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "FzGgu4Seh0rk",
        "outputId": "aa403e8e-5239-4419-a498-c3a97599d613"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/pyLDAvis/_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
            "  default_term_info = default_term_info.sort_values(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el162941397055959697121231081218\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el162941397055959697121231081218_data = {\"mdsDat\": {\"x\": [-0.052982033469440755, -0.06910151091131557, 0.12208354438075632], \"y\": [0.08297905391184547, -0.07598280106333337, -0.006996252848512128], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [39.69851516137123, 37.04469771249887, 23.25678712612989]}, \"tinfo\": {\"Term\": [\"ai\", \"educ\", \"onlin\", \"languag\", \"learn\", \"prompt\", \"model\", \"nlp\", \"control\", \"higher\", \"word\", \"student\", \"person\", \"process\", \"technolog\", \"education.\", \"academ\", \"glove\", \"brain\", \"review\", \"natur\", \"method\", \"empir\", \"automat\", \"sourc\", \"vector\", \"overall,\", \"schedul\", \"paper\", \"applic\", \"prompt\", \"glove\", \"vector\", \"co-occurr\", \"like\", \"templat\", \"search\", \"cooper\", \"co-evolutionari\", \"log-bilinear\", \"paramet\", \"meaning\", \"prediction-bas\", \"organ\", \"syntax\", \"model,\", \"learning,\", \"task\", \"method\", \"paper\", \"quantum\", \"problems,\", \"qgdecc,\", \"regress\", \"analog\", \"appropri\", \"impact\", \"fundament\", \"space\", \"similar\", \"word\", \"topic\", \"evolut\", \"new\", \"model\", \"specif\", \"differenti\", \"weight\", \"discuss\", \"outperform\", \"algorithm\", \"design\", \"list\", \"engin\", \"processing,\", \"adapt\", \"call\", \"entiti\", \"answer\", \"passag\", \"train\", \"articl\", \"includ\", \"languag\", \"optim\", \"effect\", \"provid\", \"mention\", \"natur\", \"generat\", \"perform\", \"learn\", \"text\", \"differ\", \"process\", \"use\", \"brain\", \"schedul\", \"soldier\", \"nlp\", \"activ\", \"symbol\", \"nlp,\", \"(nlp)\", \"deep\", \"earli\", \"updat\", \"explain\", \"linguist\", \"strength\", \"translation,\", \"cases.\", \"text.\", \"layer\", \"(nlp),\", \"weak\", \"transform\", \"attack\", \"mechan\", \"test\", \"score\", \"note\", \"problem\", \"data,\", \"phenomena.\", \"suitabl\", \"statist\", \"approach\", \"languag\", \"process\", \"focus\", \"metaheurist\", \"highlight\", \"develop\", \"natur\", \"human\", \"level\", \"differ\", \"studi\", \"optim\", \"algorithm\", \"algorithm,\", \"articl\", \"train\", \"perform\", \"text\", \"discuss\", \"model\", \"research\", \"propos\", \"includ\", \"provid\", \"use\", \"learn\", \"base\", \"ai\", \"onlin\", \"educ\", \"technolog\", \"education.\", \"academ\", \"empir\", \"automat\", \"sourc\", \"person\", \"potenti\", \"test-b\", \"experience.\", \"performance,\", \"assessment,\", \"higher\", \"quality.\", \"aie\", \"actual\", \"system.\", \"good\", \"appli\", \"optimizer,\", \"help\", \"recommend\", \"enhanc\", \"recommendations,\", \"recommendation,\", \"implic\", \"practic\", \"student\", \"review\", \"learn\", \"control,\", \"overall,\", \"education,\", \"control\", \"applic\", \"need\", \"mode\", \"content\", \"common\", \"articl\", \"support\", \"research\", \"includ\", \"predict\", \"relat\", \"improv\", \"provid\", \"use\", \"effect\", \"function\", \"text\", \"refer\", \"algorithm\"], \"Freq\": [10.0, 11.0, 8.0, 33.0, 19.0, 11.0, 29.0, 10.0, 9.0, 5.0, 16.0, 6.0, 5.0, 15.0, 3.0, 3.0, 3.0, 6.0, 5.0, 6.0, 15.0, 10.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 10.0, 6.0, 10.595200840097823, 6.151831464966319, 5.423614789494816, 3.210416340493483, 3.204953867932109, 3.2006318629872514, 3.198333225501731, 2.469810522340347, 2.469644563723275, 2.4687047953943373, 2.467448156745871, 2.466469222183304, 2.4637848968717093, 2.4639855961592882, 2.4635472441320627, 2.4564829389987133, 2.4229103958777443, 5.408762157101371, 8.352739682621795, 8.158261851679935, 1.7288856210861205, 1.7285781550882258, 1.728485771458056, 1.728094109121767, 1.7276205738677224, 1.7276575273197905, 1.727355371997642, 1.7272146390903653, 1.7273163164030911, 1.727079770054225, 12.817062771466608, 4.512395325700254, 3.95130605852089, 3.949568582439228, 20.95241259775179, 4.694637802278967, 4.691546988994625, 4.693777030251756, 12.850248299092566, 3.2104400172561856, 12.834767679292122, 3.944977060700246, 4.832997280053441, 5.420907672533143, 3.9427350704226862, 3.2101735983562465, 3.2080221108445297, 3.2054468756638896, 3.1993586284770776, 3.1989439532125545, 6.152384217800246, 10.59192592338761, 7.539174252118998, 10.657899563071178, 6.915111434496408, 4.668706657722445, 5.435586380295896, 3.9329439545717713, 5.406928646299964, 3.92618125156518, 4.677028486616885, 5.4585001759158995, 4.693186660131293, 4.696412895647166, 4.68661160100107, 3.985452375900615, 5.390990885100452, 4.6344278998305395, 3.9253508828369106, 9.054760513368832, 3.188805050684071, 3.1829109115521486, 3.1810327160873046, 3.1789550552729624, 3.1738674499455337, 2.452498122681718, 2.4519591945635075, 2.450708096682957, 2.4500085225586283, 2.4484301445756396, 2.4405772804899453, 1.71822280914491, 1.7174991794168377, 1.7173366750378984, 1.717028081906609, 1.7168653710418007, 1.7167927912588308, 1.7162380669716384, 1.7162351761694716, 1.7155154696729114, 1.7153244702440436, 1.7152673769012523, 1.71518560849711, 1.7151285151543187, 1.7151705350286697, 1.7152030565530445, 3.916661131523972, 8.33338808881698, 22.273354828888124, 10.53585969967612, 3.192321092062189, 3.1878174287724232, 4.6412097217134916, 6.118370396554594, 9.806102296954043, 3.1767780747556085, 3.1700098809400137, 9.025157047295421, 6.120967575815457, 8.290396904938094, 11.991510741538685, 6.12199091978245, 11.183610826534778, 6.094259454597813, 5.403006710791986, 5.41349412808084, 6.846220695790162, 7.593611165785306, 4.592220123338814, 3.9026477615352855, 5.380877207234571, 4.664053666377806, 3.9107387038279904, 3.19436881242552, 3.1935302733113233, 10.36746117347095, 8.454136878342432, 10.32901942438733, 3.384705367747472, 3.3797544421478927, 3.3286965959443986, 2.7469986165722293, 2.7452034652917785, 2.7388159519907296, 4.257845794529723, 2.1147216535637225, 2.114420257930631, 2.1127579799938014, 2.1121625777818496, 2.109306773137978, 4.661673423402056, 1.482530766555376, 1.482510025350518, 1.4823841521635366, 1.4823536885189017, 1.4815039472823799, 1.480975176191034, 1.4804508126057203, 1.4792317482901975, 1.4791482649406444, 1.4783152463005393, 1.4779319229082595, 1.4778092905345372, 1.4775395252388541, 1.4769662901895941, 4.658574687396286, 3.931744534537041, 10.369830856125963, 2.291930878158713, 3.387335093258392, 2.6374775362357914, 5.2921412348168335, 3.383744272167369, 2.123761578068505, 2.114566353792349, 2.1128845013434345, 2.1106509328453003, 8.615901683270113, 2.3393216793433314, 4.087862805707222, 5.428694586709624, 3.5961196910687967, 2.5441128174234096, 3.3803100471730234, 4.034765580536075, 3.3840076854690646, 2.769431785216405, 2.7482161252973873, 2.7527527453299303, 2.460020194977688, 2.8175728994867577], \"Total\": [10.0, 11.0, 8.0, 33.0, 19.0, 11.0, 29.0, 10.0, 9.0, 5.0, 16.0, 6.0, 5.0, 15.0, 3.0, 3.0, 3.0, 6.0, 5.0, 6.0, 15.0, 10.0, 3.0, 3.0, 3.0, 5.0, 5.0, 5.0, 10.0, 6.0, 11.089861371215964, 6.636922317474457, 5.896944003732755, 3.6722727599561638, 3.672082961495994, 3.6715568459364003, 3.6716224630836196, 2.930347813505145, 2.930308518382599, 2.930126819017724, 2.9303490283326634, 2.9298031104892543, 2.9294071516397913, 2.9297781088666417, 2.9297837379677825, 2.92853462774918, 2.9249437226857014, 6.6319601453964605, 10.333035742775849, 10.204942974677008, 2.1884924894957933, 2.188450314484405, 2.1884489610379516, 2.188360315006293, 2.188373858734259, 2.188474654382262, 2.188364642333636, 2.1882890242954836, 2.1884355435471177, 2.1883032235983086, 16.258925443664815, 5.768161650277217, 5.146604499082856, 5.150361845157486, 29.472634044199015, 6.529170072222385, 6.624108789869851, 6.630237395772536, 21.217001278448613, 4.408836470129186, 27.643851320317566, 5.786119817874716, 7.822043207426552, 9.27999174207463, 5.882460478899666, 4.408780418570922, 4.408602326485397, 4.408275300986056, 4.408340231503718, 4.407433365125932, 12.52578849740723, 30.3914384331925, 18.348746046063194, 33.195217578644275, 16.115577120324662, 8.43435976833186, 14.134405627209777, 6.39655825182656, 15.472204078309582, 6.419848287324055, 10.94813596512451, 19.022699844467382, 12.859433533542063, 15.26331399745158, 15.46938834435377, 11.280198765197671, 5.863337001920168, 5.122585194520704, 4.389813261511679, 10.28979805283546, 3.6529681844811637, 3.652048371702116, 3.6529438324540093, 3.6525489417541097, 3.652495071087256, 2.9157282351244485, 2.9157173151318436, 2.915650624775643, 2.9157040944483303, 2.9154233699180434, 2.915637546355383, 2.1788754726235537, 2.178823680430117, 2.178894317146402, 2.17888773515652, 2.178789943509943, 2.1788955776553482, 2.1787660970196927, 2.1787984561359632, 2.1786540868998836, 2.178703979760066, 2.1787329368745527, 2.1786601386799243, 2.1786012077203285, 2.178667339060858, 2.1787241193072915, 5.13089301548967, 11.565980984994935, 33.195217578644275, 15.46938834435377, 4.28907992445761, 4.288573199157315, 6.502356690301082, 8.823823229270966, 15.472204078309582, 4.392976886891015, 4.391869073285426, 15.26331399745158, 10.73206078924183, 16.115577120324662, 27.643851320317566, 11.580954509501321, 30.3914384331925, 12.52578849740723, 10.94813596512451, 12.859433533542063, 21.217001278448613, 29.472634044199015, 10.416250371692948, 7.247876398514952, 18.348746046063194, 14.134405627209777, 11.280198765197671, 19.022699844467382, 5.877839923205154, 10.881348855748548, 8.974720018765687, 11.629538274586745, 3.885155388019782, 3.8859419835414175, 3.8943632886337634, 3.2495972866572362, 3.2498803217421193, 3.25089752481267, 5.220181396715529, 2.6131790354252873, 2.6132454702959693, 2.6134877683510056, 2.613585579764094, 2.6140385683982945, 5.8929976488921, 1.9767430793870289, 1.9767458111880747, 1.9767661306782962, 1.976771173796578, 1.9769051105859226, 1.9769889319623553, 1.977071351263048, 1.9772909706212156, 1.9772840658391846, 1.9774159632246133, 1.97747373061272, 1.97749199653911, 1.9775389240763168, 1.9776511816561526, 6.640260165709341, 6.014604462804147, 19.022699844467382, 3.3218696312826275, 5.363497244568169, 4.009698118009732, 9.502268538749803, 6.105989992390232, 3.353505212646077, 3.350150356033085, 3.350400884935581, 3.3507886070656525, 30.3914384331925, 4.056133286704184, 10.416250371692948, 18.348746046063194, 9.028550146668035, 4.767206704878223, 9.069407040847224, 14.134405627209777, 11.280198765197671, 8.43435976833186, 8.432980882272961, 12.859433533542063, 6.265137806054176, 27.643851320317566], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.4965, -5.0401, -5.1661, -5.6905, -5.6922, -5.6935, -5.6942, -5.9527, -5.9528, -5.9532, -5.9537, -5.9541, -5.9552, -5.9551, -5.9553, -5.9581, -5.9719, -5.1688, -4.7343, -4.7578, -6.3094, -6.3096, -6.3096, -6.3098, -6.3101, -6.3101, -6.3103, -6.3104, -6.3103, -6.3104, -4.3061, -5.35, -5.4828, -5.4833, -3.8146, -5.3104, -5.3111, -5.3106, -4.3035, -5.6905, -4.3047, -5.4844, -5.2814, -5.1666, -5.485, -5.6905, -5.6912, -5.692, -5.6939, -5.694, -5.04, -4.4968, -4.8368, -4.4906, -4.9232, -5.316, -5.1639, -5.4875, -5.1692, -5.4892, -5.3142, -5.1597, -5.3108, -5.3101, -5.3122, -5.4742, -5.1029, -5.2542, -5.4202, -4.5844, -5.628, -5.6299, -5.6305, -5.6311, -5.6327, -5.8906, -5.8908, -5.8913, -5.8916, -5.8922, -5.8954, -6.2464, -6.2468, -6.2469, -6.2471, -6.2472, -6.2472, -6.2475, -6.2475, -6.248, -6.2481, -6.2481, -6.2482, -6.2482, -6.2482, -6.2481, -5.4224, -4.6674, -3.6843, -4.4329, -5.6269, -5.6283, -5.2527, -4.9764, -4.5047, -5.6318, -5.6339, -4.5877, -4.976, -4.6726, -4.3035, -4.9758, -4.3732, -4.9803, -5.1007, -5.0988, -4.864, -4.7604, -5.2633, -5.426, -5.1048, -5.2478, -5.4239, -5.6263, -5.6265, -3.9835, -4.1875, -3.9872, -5.1029, -5.1043, -5.1196, -5.3116, -5.3123, -5.3146, -4.8734, -5.5732, -5.5734, -5.5742, -5.5744, -5.5758, -4.7828, -5.9284, -5.9284, -5.9285, -5.9285, -5.9291, -5.9294, -5.9298, -5.9306, -5.9307, -5.9312, -5.9315, -5.9316, -5.9318, -5.9322, -4.7834, -4.9531, -3.9832, -5.4928, -5.1021, -5.3523, -4.6559, -5.1032, -5.569, -5.5733, -5.5741, -5.5752, -4.1685, -5.4723, -4.9141, -4.6304, -5.0423, -5.3884, -5.1042, -4.9272, -5.1031, -5.3035, -5.3112, -5.3095, -5.422, -5.2863], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.8782, 0.848, 0.8402, 0.7894, 0.7878, 0.7866, 0.7859, 0.7529, 0.7528, 0.7525, 0.7519, 0.7517, 0.7508, 0.7507, 0.7505, 0.7481, 0.7356, 0.72, 0.7111, 0.7, 0.6881, 0.688, 0.6879, 0.6877, 0.6874, 0.6874, 0.6873, 0.6872, 0.6872, 0.6872, 0.686, 0.6783, 0.6596, 0.6584, 0.5826, 0.594, 0.5789, 0.5785, 0.4224, 0.6067, 0.1566, 0.5408, 0.4424, 0.3863, 0.5238, 0.6066, 0.606, 0.6052, 0.6033, 0.6034, 0.2129, -0.1302, 0.0344, -0.2122, 0.0778, 0.3324, -0.0318, 0.4375, -0.1275, 0.4321, 0.0734, -0.3246, -0.0841, -0.2548, -0.2703, -0.1165, 0.9091, 0.8929, 0.8812, 0.8652, 0.8572, 0.8556, 0.8547, 0.8542, 0.8526, 0.82, 0.8198, 0.8193, 0.819, 0.8185, 0.8152, 0.7555, 0.7551, 0.755, 0.7548, 0.7548, 0.7547, 0.7544, 0.7544, 0.7541, 0.7539, 0.7539, 0.7539, 0.7538, 0.7538, 0.7538, 0.723, 0.6652, 0.594, 0.609, 0.6977, 0.6964, 0.6559, 0.6269, 0.537, 0.6689, 0.667, 0.4676, 0.4315, 0.3284, 0.1578, 0.3556, -0.0067, 0.2726, 0.2868, 0.1279, -0.1381, -0.3631, 0.174, 0.374, -0.2337, -0.1157, -0.0663, -0.7912, 0.383, 1.4102, 1.3988, 1.34, 1.3207, 1.319, 1.3016, 1.2906, 1.2898, 1.2872, 1.2548, 1.2469, 1.2468, 1.2459, 1.2456, 1.244, 1.2242, 1.1709, 1.1709, 1.1708, 1.1707, 1.1701, 1.1697, 1.1693, 1.1684, 1.1683, 1.1677, 1.1674, 1.1673, 1.1671, 1.1667, 1.1041, 1.0335, 0.8518, 1.0874, 0.999, 1.0397, 0.8733, 0.8683, 1.0018, 0.9984, 0.9975, 0.9964, 0.198, 0.9082, 0.5232, 0.2407, 0.538, 0.8306, 0.4716, 0.2049, 0.2546, 0.3449, 0.3374, -0.0829, 0.5237, -0.825]}, \"token.table\": {\"Topic\": [2, 2, 3, 2, 3, 1, 2, 3, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 3, 2, 3, 1, 2, 2, 1, 2, 2, 1, 1, 2, 3, 2, 3, 1, 3, 2, 3, 1, 2, 2, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 3, 1, 3, 3, 1, 2, 3, 3, 1, 2, 3, 3, 1, 2, 1, 2, 3, 2, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 3, 3, 2, 3, 1, 2, 3, 1, 2, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 2, 1, 2, 3, 1, 1, 2, 1, 2, 1, 2, 3, 1, 1, 2, 1, 2, 3, 2, 3, 1, 2, 2, 3, 1, 2, 3, 1, 1, 2, 1, 3, 1, 2, 1, 2, 2, 2, 3, 1, 2, 3, 3, 1, 1, 2, 1, 2, 3, 1, 2, 3, 1, 1, 2, 1, 2, 3, 3, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 1, 1, 2, 1, 2, 1, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 3, 3, 1, 3, 1, 1, 3, 1, 2, 3, 1, 2, 3, 2, 2, 1, 1, 2, 3, 1, 1, 2, 3, 1, 2, 2, 1, 3, 1, 2, 3, 2, 1, 2, 3, 2, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 1, 3, 1, 2, 2, 2, 2, 1, 2, 3, 1, 2, 1, 2, 1, 2], \"Freq\": [0.8213442305195429, 0.9178995171388811, 0.7703441558099918, 0.821249966737965, 0.505876737000176, 0.6804602895084602, 0.22682009650282006, 0.9190037129190158, 0.5058819370402381, 0.47026732452599007, 0.43409291494706775, 0.10852322873676694, 0.3453946733594622, 0.5180920100391934, 0.08634866833986556, 0.9139206228485962, 0.6805282356749214, 0.2268427452249738, 0.5058197260656396, 0.3275472122444613, 0.16377360612223066, 0.49132081836669195, 0.17292091372056462, 0.6916836548822585, 0.08646045686028231, 0.9138785299592778, 0.36194403973936856, 0.36194403973936856, 0.2961360325140288, 0.7650996523840367, 0.9179507624686172, 0.9231109157865329, 0.34026105272179835, 0.5103915790826975, 0.8527567148814001, 0.680487777719712, 0.226829259239904, 0.9179046830022956, 0.6825219895630347, 0.8169327814407259, 0.29843720904725124, 0.5968744180945025, 0.2984717454249441, 0.5969434908498882, 0.4209521109288996, 0.5261901386611245, 0.30103529367402776, 0.6020705873480555, 0.6825128371391838, 0.9180202383587148, 0.821356344529433, 0.6913095694359868, 0.1728273923589967, 0.1728273923589967, 0.22665911907272449, 0.6799773572181734, 0.3275828565693413, 0.5896491418248143, 0.13103314262773652, 0.7548185210433778, 0.3019274084173511, 0.6127161812072323, 0.3299240975731251, 0.09426402787803574, 0.6859349838942163, 0.0859879366135484, 0.859879366135484, 0.24939533365578243, 0.7481860009673473, 0.7720135845327206, 0.5928132232126607, 0.11856264464253215, 0.35568793392759646, 0.9231913173727476, 0.53879358290056, 0.21551743316022404, 0.21551743316022404, 0.5057104921765065, 0.6805382593344275, 0.22684608644480916, 0.7772114606266743, 0.19430286515666859, 0.7652608993314367, 0.6859532424787343, 0.6994507103710302, 0.2331502367903434, 0.4743281238083242, 0.2371640619041621, 0.35574609285624315, 0.9139560532429655, 0.6230676833746946, 0.15576692084367366, 0.3115338416873473, 0.9040334831405976, 0.5058411729754779, 0.5057424601933144, 0.16969292363250862, 0.848464618162543, 0.15379039441062967, 0.7689519720531484, 0.15379039441062967, 0.22763607133560795, 0.6829082140068239, 0.9139244718682865, 0.5056790477421764, 0.44104316654711945, 0.22052158327355972, 0.33078237491033957, 0.43599709647278245, 0.272498185295489, 0.272498185295489, 0.33137303510481314, 0.6627460702096263, 0.9178967443539474, 0.26284386763607664, 0.15770632058164596, 0.5256877352721533, 0.6837738396428317, 0.227693490701427, 0.6830804721042809, 0.8169750061359753, 0.685940663117398, 0.639219174250125, 0.127843834850025, 0.25568766970005, 0.682564313264252, 0.6826397285331626, 0.9179371292317431, 0.6253362890672942, 0.15633407226682355, 0.3126681445336471, 0.6995333554268087, 0.23317778514226956, 0.7742158450959634, 0.19355396127399085, 0.29849406555713537, 0.5969881311142707, 0.7125253877378954, 0.2714382429477697, 0.03392978036847121, 0.6829354111264734, 0.323160163522499, 0.646320327044998, 0.2981954512040111, 0.5963909024080222, 0.7766444611577944, 0.1941611152894486, 0.09718363711952925, 0.8746527340757633, 0.8212554415282732, 0.9179647336075298, 0.8913927101093296, 0.43436235312800137, 0.496414117860573, 0.062051764732571624, 0.5057986396703144, 0.6826455539234273, 0.6804516385049988, 0.22681721283499962, 0.18644551388792835, 0.18644551388792835, 0.5593365416637851, 0.7839338269553833, 0.09799172836942291, 0.09799172836942291, 0.682512554191532, 0.6806682600666571, 0.22688942002221904, 0.4566987490772486, 0.4566987490772486, 0.09133974981544972, 0.7652322600358558, 0.1915642243063023, 0.7662568972252092, 0.9179923727420108, 0.7653513107549119, 0.505650343839992, 0.33227926425231663, 0.22151950950154442, 0.44303901900308884, 0.6827319988211479, 0.9179954066685332, 0.9138886940968528, 0.32321898504959107, 0.7110817671091004, 0.6799875688664573, 0.33999378443322864, 0.99189697975403, 0.27594289555072826, 0.5518857911014565, 0.13797144777536413, 0.35374674619317775, 0.35374674619317775, 0.2829973969545422, 0.9138892592913965, 0.5058826361542601, 0.9138710823087083, 0.5057442262731163, 0.5056910479284574, 0.505695718997061, 0.6384536340341452, 0.3192268170170726, 0.9139262790891219, 0.4195328887151935, 0.6292993330727903, 0.19200767345562, 0.48001918363905, 0.38401534691124, 0.16626197220187228, 0.16626197220187228, 0.6650478888074891, 0.9760696621206367, 0.9179769342599052, 0.8170774719251619, 0.9139501228313895, 0.9112004911622499, 0.9228220751661105, 0.91389486242684, 0.7657941123745473, 0.15315882247490945, 0.15315882247490945, 0.19489784662847123, 0.7795913865138849, 0.686006711970695, 0.30119301805795307, 0.7529825451448827, 0.1863574982732921, 0.5590724948198763, 0.1863574982732921, 0.9179684487248824, 0.2465402217619311, 0.2465402217619311, 0.4930804435238622, 0.8214568085257276, 0.682644242331443, 0.5058754464126489, 0.7539249166735, 0.15078498333470003, 0.7721698877864096, 0.8170920745297284, 0.9179979566402395, 0.7653318537173185, 0.388819615339835, 0.388819615339835, 0.233291769203901, 0.9179265022515195, 0.8668273018596316, 0.17336546037192632, 0.47901176051647104, 0.47901176051647104, 0.9178962133431594, 0.6859563193991819, 0.6859375528692374, 0.35460368059657194, 0.35460368059657194, 0.26595276044742894, 0.8478968083866846, 0.9179407156515879, 0.7541208106949562, 0.3016483242779825, 0.7995608347577094, 0.18451403879024064], \"Term\": [\"(nlp)\", \"(nlp),\", \"academ\", \"activ\", \"actual\", \"adapt\", \"adapt\", \"ai\", \"aie\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm,\", \"algorithm,\", \"algorithm,\", \"analog\", \"answer\", \"answer\", \"appli\", \"applic\", \"applic\", \"applic\", \"approach\", \"approach\", \"approach\", \"appropri\", \"articl\", \"articl\", \"articl\", \"assessment,\", \"attack\", \"automat\", \"base\", \"base\", \"brain\", \"call\", \"call\", \"cases.\", \"co-evolutionari\", \"co-occurr\", \"common\", \"common\", \"content\", \"content\", \"control\", \"control\", \"control,\", \"control,\", \"cooper\", \"data,\", \"deep\", \"design\", \"design\", \"design\", \"develop\", \"develop\", \"differ\", \"differ\", \"differ\", \"differenti\", \"differenti\", \"discuss\", \"discuss\", \"discuss\", \"earli\", \"educ\", \"educ\", \"education,\", \"education,\", \"education.\", \"effect\", \"effect\", \"effect\", \"empir\", \"engin\", \"engin\", \"engin\", \"enhanc\", \"entiti\", \"entiti\", \"evolut\", \"evolut\", \"experience.\", \"explain\", \"focus\", \"focus\", \"function\", \"function\", \"function\", \"fundament\", \"generat\", \"generat\", \"generat\", \"glove\", \"good\", \"help\", \"higher\", \"higher\", \"highlight\", \"highlight\", \"highlight\", \"human\", \"human\", \"impact\", \"implic\", \"improv\", \"improv\", \"improv\", \"includ\", \"includ\", \"includ\", \"languag\", \"languag\", \"layer\", \"learn\", \"learn\", \"learn\", \"learning,\", \"level\", \"level\", \"like\", \"linguist\", \"list\", \"list\", \"list\", \"log-bilinear\", \"meaning\", \"mechan\", \"mention\", \"mention\", \"mention\", \"metaheurist\", \"metaheurist\", \"method\", \"method\", \"mode\", \"mode\", \"model\", \"model\", \"model\", \"model,\", \"natur\", \"natur\", \"need\", \"need\", \"new\", \"new\", \"nlp\", \"nlp\", \"nlp,\", \"note\", \"onlin\", \"optim\", \"optim\", \"optim\", \"optimizer,\", \"organ\", \"outperform\", \"outperform\", \"overall,\", \"overall,\", \"overall,\", \"paper\", \"paper\", \"paper\", \"paramet\", \"passag\", \"passag\", \"perform\", \"perform\", \"perform\", \"performance,\", \"person\", \"person\", \"phenomena.\", \"potenti\", \"practic\", \"predict\", \"predict\", \"predict\", \"prediction-bas\", \"problem\", \"problems,\", \"process\", \"process\", \"processing,\", \"processing,\", \"prompt\", \"propos\", \"propos\", \"propos\", \"provid\", \"provid\", \"provid\", \"qgdecc,\", \"quality.\", \"quantum\", \"recommend\", \"recommendation,\", \"recommendations,\", \"refer\", \"refer\", \"regress\", \"relat\", \"relat\", \"research\", \"research\", \"research\", \"review\", \"review\", \"review\", \"schedul\", \"score\", \"search\", \"similar\", \"soldier\", \"sourc\", \"space\", \"specif\", \"specif\", \"specif\", \"statist\", \"statist\", \"strength\", \"student\", \"student\", \"studi\", \"studi\", \"studi\", \"suitabl\", \"support\", \"support\", \"support\", \"symbol\", \"syntax\", \"system.\", \"task\", \"task\", \"technolog\", \"templat\", \"test\", \"test-b\", \"text\", \"text\", \"text\", \"text.\", \"topic\", \"topic\", \"train\", \"train\", \"transform\", \"translation,\", \"updat\", \"use\", \"use\", \"use\", \"vector\", \"weak\", \"weight\", \"weight\", \"word\", \"word\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 1, 2]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el162941397055959697121231081218\", ldavis_el162941397055959697121231081218_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el162941397055959697121231081218\", ldavis_el162941397055959697121231081218_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el162941397055959697121231081218\", ldavis_el162941397055959697121231081218_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store GPT-3.5 result\n",
        "f = open(\"/content/result.txt\", \"a\")\n",
        "for data in datas:\n",
        "  for item in data:\n",
        "    f.writelines(item)\n",
        "    f.writelines('\\n\\n')\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msfQ60sEiNo8",
        "outputId": "fbf916d3-cc42-42f8-9654-15a63a6b3a1e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    }
  ]
}